# -*- coding: utf-8 -*-
"""project02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WHgKAEDsI8eeOmamdgDeNu0hD4LNKQtf
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **==== DATA PROCESSING ====**"""

import pandas as pd
import numpy as np # Import numpy untuk NaN
import os
import glob

# --- Konfigurasi ---
folder_path = r'/content/drive/MyDrive/project02'
file_pattern = "*_20*.xlsx"
location_mapping = {
    'bek': 'Kota Bekasi', 'bog': 'Kota Bogor', 'jkt': 'Jakarta Pusat',
    'tgr': 'Kota Tangerang', 'dep': 'Kota Depok'
}
# Pastikan nama kolom ID ini cocok persis dengan di Excel Anda
kolom_id_list = ['No', 'Komoditas (Rp)']

# Nama file output BARU (menunjukkan format wide dan sudah diimputasi)
output_wide_parquet = 'data_pangan_jabodetabek_wide_imputed.parquet'
output_wide_csv = 'data_pangan_jabodetabek_wide_imputed.csv' # CSV bisa sangat besar
# --- Akhir Konfigurasi ---

all_processed_long_data_list = [] # List untuk data long sebelum pivot
file_paths = glob.glob(os.path.join(folder_path, file_pattern))

if not file_paths:
    print(f"Error: Tidak ada file yang cocok '{file_pattern}' di '{folder_path}'.")
else:
    print(f"Ditemukan {len(file_paths)} file. Tahap 1: Membaca & Melt...")

    # Tahap 1: Baca semua file, melt ke format long, lakukan pembersihan dasar
    for file_path in file_paths:
        file_name = os.path.basename(file_path)
        print(f"  Memproses file: {file_name}...")

        try:
            file_prefix = file_name.split('_')[0]
            if file_prefix in location_mapping:
                location_name = location_mapping[file_prefix]
            else:
                print(f"    Peringatan: Awalan '{file_prefix}' tidak di mapping. Melewati.")
                continue

            try:
                df_wide_raw = pd.read_excel(file_path, header=0) # Coba header 0
                if not all(col in df_wide_raw.columns for col in kolom_id_list):
                     print("    Info: Kolom ID tidak di header 1, mencoba header 2...")
                     df_wide_raw = pd.read_excel(file_path, header=1)
                # print(f"    Kolom: {df_wide_raw.columns.tolist()}") # Uncomment untuk debug kolom
            except Exception as read_err:
                print(f"    Error saat membaca file {file_name}: {read_err}")
                continue

            missing_id_cols = [col for col in kolom_id_list if col not in df_wide_raw.columns]
            if missing_id_cols:
                print(f"    Error: Kolom ID {missing_id_cols} tidak ditemukan di file {file_name}. Melewati.")
                continue

            # Melt
            df_long = pd.melt(df_wide_raw,
                              id_vars=kolom_id_list,
                              var_name='Tanggal_Str',
                              value_name='Harga_Raw') # Simpan harga mentah

            # --- Pembersihan Dasar (Tanggal, Lokasi, Komoditas) ---
            # 1. Konversi Tanggal
            date_format_string = '%d/ %m/ %Y'
            df_long['Tanggal'] = pd.to_datetime(df_long['Tanggal_Str'],
                                              format=date_format_string,
                                              errors='coerce')

            # Hapus baris dengan tanggal tidak valid SEKARANG
            rows_before_date_drop = len(df_long)
            df_long.dropna(subset=['Tanggal'], inplace=True)
            if len(df_long) < rows_before_date_drop:
                 print(f"    Info: {rows_before_date_drop - len(df_long)} baris dihapus karena tanggal tidak valid.")

            if df_long.empty:
                 print(f"    Info: Tidak ada data dengan tanggal valid di file {file_name}.")
                 continue

            # 2. Tambah Lokasi
            df_long['Lokasi'] = location_name

            # 3. Rename & Clean Komoditas
            komoditas_col_name = kolom_id_list[1]
            df_long.rename(columns={komoditas_col_name: 'Nama_Komoditas'}, inplace=True)
            df_long['Nama_Komoditas'] = df_long['Nama_Komoditas'].astype(str).str.strip()

            # 4. Pilih Kolom yang Relevan untuk tahap selanjutnya
            #    Kita masih butuh Harga_Raw untuk pivot
            df_to_append = df_long[['Tanggal', 'Lokasi', 'Nama_Komoditas', 'Harga_Raw']].copy()

            all_processed_long_data_list.append(df_to_append)
            print(f"    Info: Berhasil memproses dasar file {file_name}.")

        except Exception as e:
            print(f"    FATAL Error saat memproses file {file_name}: {e}")
            import traceback
            traceback.print_exc()

    # --- Gabungkan Semua Data Long ---
    if all_processed_long_data_list:
        print("\nTahap 2: Menggabungkan semua data long...")
        combined_long_df = pd.concat(all_processed_long_data_list, ignore_index=True)
        print(f"Total baris data long gabungan: {len(combined_long_df)}")
        print(combined_long_df.info())

        # --- Tahap 3: Pivot ke Format Wide ---
        print("\nTahap 3: Melakukan Pivot ke format wide (Tanggal sebagai index)...")
        try:
            # Membuat nama kolom gabungan 'Lokasi_NamaKomoditas'
            combined_long_df['Pivot_Column'] = combined_long_df['Lokasi'] + "_" + combined_long_df['Nama_Komoditas']

            df_wide_pivoted = combined_long_df.pivot_table(index='Tanggal',
                                                         columns='Pivot_Column',
                                                         values='Harga_Raw',
                                                         aggfunc='first') # 'first' untuk handle duplikat tanggal-kolom jika ada
            print("Pivot berhasil.")
            print(f"Dimensi data wide setelah pivot: {df_wide_pivoted.shape}")

        except Exception as pivot_err:
            print(f"  FATAL Error saat melakukan pivot: {pivot_err}")
            df_wide_pivoted = None # Set ke None jika pivot gagal

        if df_wide_pivoted is not None:
            # --- Tahap 4: Pastikan Frekuensi Harian & Alignment ---
            print("\nTahap 4: Memastikan frekuensi harian (asfreq)...")
            # Tentukan rentang tanggal penuh
            start_date = df_wide_pivoted.index.min()
            end_date = df_wide_pivoted.index.max()
            daily_index = pd.date_range(start=start_date, end=end_date, freq='D')

            # Reindex ke frekuensi harian
            df_wide_aligned = df_wide_pivoted.reindex(daily_index)
            print(f"Dimensi data wide setelah alignment: {df_wide_aligned.shape}")
            print(f"Jumlah missing values sebelum imputasi: {df_wide_aligned.isna().sum().sum()}") # Total NaN


            # --- Tahap 5: Cleaning Harga & Imputasi di Format Wide ---
            print("\nTahap 5: Membersihkan harga (koma, '-') dan melakukan imputasi...")
            # Buat salinan agar kita tidak bekerja pada slice yang mungkin copy
            df_wide_imputed = df_wide_aligned.copy()

            total_imputed = 0
            total_remaining_nans = 0
            for col in df_wide_imputed.columns:
                print(f"  Memproses kolom: {col}")
                nans_before = df_wide_imputed[col].isna().sum()

                # --- Cleaning Harga ---
                # Pastikan string, ganti '-', hapus ',', konversi ke numerik
                harga_str = df_wide_imputed[col].astype(str)
                harga_cleaned_hyphen = harga_str.replace('-', np.nan, regex=False).replace('nan', np.nan)
                harga_cleaned_comma = harga_cleaned_hyphen.str.replace(',', '', regex=False)
                # Assign hasil konversi ke kolom DataFrame (menghindari inplace)
                df_wide_imputed[col] = pd.to_numeric(harga_cleaned_comma, errors='coerce')

                # --- Imputasi ---
                # Gunakan .ffill() dan .bfill() dan assign hasilnya kembali
                df_wide_imputed[col] = df_wide_imputed[col].ffill() # Mengisi maju
                df_wide_imputed[col] = df_wide_imputed[col].bfill() # Mengisi mundur (untuk NaN di awal)

                nans_after = df_wide_imputed[col].isna().sum()
                if nans_before > nans_after:
                    imputed_count = nans_before - nans_after
                    print(f"    {imputed_count} nilai diimputasi.")
                    total_imputed += imputed_count
                if nans_after > 0:
                    print(f"    Peringatan: Masih ada {nans_after} NaN di kolom {col} setelah imputasi (mungkin kolom kosong total?).")
                    total_remaining_nans += nans_after

            print(f"\nTotal nilai yang diimputasi di semua kolom: {total_imputed}")
            if total_remaining_nans > 0:
                print(f"Jumlah total missing values SETELAH imputasi: {total_remaining_nans} (Perlu diperiksa!)")
            else:
                print("Jumlah total missing values SETELAH imputasi: 0")

            # Tampilkan info hasil akhir
            print("\n--- Hasil Akhir (Data Wide Siap untuk LSTM) ---")
            print(df_wide_imputed.info())
            if not df_wide_imputed.empty:
                print("\nContoh 5 baris pertama:")
                print(df_wide_imputed.head())
                print("\nContoh 5 baris terakhir:")
                print(df_wide_imputed.tail())
                print(f"\nTotal baris: {len(df_wide_imputed)}")
                print(f"Total kolom (time series): {len(df_wide_imputed.columns)}")
                print("\nRentang Tanggal:")
                print(f"Dari {df_wide_imputed.index.min().strftime('%Y-%m-%d')} hingga {df_wide_imputed.index.max().strftime('%Y-%m-%d')}")
            else:
                 print("\nDataFrame wide kosong setelah proses.")

            # --- Simpan hasil ke file ---
            output_parquet_path_wide = os.path.join(folder_path, output_wide_parquet)
            output_csv_path_wide = os.path.join(folder_path, output_wide_csv)
            try:
                print(f"\nMenyimpan hasil ke {output_parquet_path_wide}...")
                df_wide_imputed.to_parquet(output_parquet_path_wide, engine='pyarrow')
                print("OK Parquet.")
            except ImportError:
                 print("  Gagal Parquet: 'pyarrow' tidak ditemukan.")
            except Exception as e:
                print(f"  Gagal Parquet: {e}")

            try:
                print(f"\nMenyimpan hasil ke {output_csv_path_wide}...")
                df_wide_imputed.to_csv(output_csv_path_wide)
                print("OK CSV (Mungkin sangat besar).")
            except Exception as e:
                print(f"  Gagal CSV: {e}")

        else:
            print("\nPivot gagal, proses tidak dapat dilanjutkan ke alignment dan imputasi.")

    else:
        print("\nTidak ada data long yang berhasil diproses untuk digabungkan.")

print("\nProses selesai.")

# --- Cek dan Set Nama Index ---

# Cek nama index saat ini
current_index_name = df_wide_imputed.index.name
print(f"\nNama index saat ini: {current_index_name}")

# Jika nama index kosong (None) atau ingin menggantinya
if current_index_name is None:
    new_index_name = 'Tanggal' # Nama yang diinginkan
    print(f"Menetapkan nama index menjadi: '{new_index_name}'")
    df_wide_imputed.index.name = new_index_name
    # Verifikasi lagi
    print(f"Nama index setelah ditetapkan: {df_wide_imputed.index.name}")
elif current_index_name != 'Tanggal':
    print(f"Nama index saat ini adalah '{current_index_name}', mengubah menjadi 'Tanggal'")
    df_wide_imputed.index.name = 'Tanggal'
    print(f"Nama index setelah diubah: {df_wide_imputed.index.name}")
else:
    print("Nama index sudah 'Tanggal'.")

# Anda bisa melihat DataFrame lagi jika mau
df_wide_imputed.head()

df_wide_imputed.info()

import pandas as pd
df_gab = pd.read_csv('/content/drive/MyDrive/project02/data_pangan_jabodetabek_wide_imputed.csv')
df_gab.head(15)

df_gab.shape

df_gab.info()



"""# **EXPLORATORY DATA ANALYSIS**"""

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller # Untuk tes stasioneritas
import os

# --- Konfigurasi EDA ---
# 1. Path ke folder tempat file wide gabungan disimpan
folder_path = r'/content/drive/MyDrive/project02' # Sesuaikan

# 2. Nama file Parquet/CSV wide yang sudah diimputasi
parquet_file_wide = 'data_pangan_jabodetabek_wide_imputed.parquet'
csv_file_wide = 'data_pangan_jabodetabek_wide_imputed.csv'

wide_parquet_path = os.path.join(folder_path, parquet_file_wide)
wide_csv_path = os.path.join(folder_path, csv_file_wide)

# 3. Pilih Beberapa Kolom Kunci untuk Visualisasi & Analisis Detail
#    Ganti dengan nama kolom (Lokasi_NamaKomoditas) yang relevan dari data Anda
#    Sertakan target potensial Anda dan beberapa fitur yang mungkin terkait

"""
KOMODITAS: 'Beras', 'Beras Kualitas Bawah I', 'Beras Kualitas Bawah II', 'Beras Kualitas Medium I', 'Beras Kualitas Medium II',
'Beras Kualitas Super I', 'Beras Kualitas Super II', 'Daging Ayam', 'Daging Ayam Ras Segar', 'Daging Sapi', 'Daging Sapi Kualitas I', 'Daging Sapi Kualitas II',
'Telur Ayam', 'Telur Ayam Ras Segar', 'Bawang Merah', 'Bawang Merah Ukuran Sedang', 'Bawang Putih', 'Bawang Putih Ukuran Sedang', 'Cabai Merah', 'Cabai Merah Besar',
'Cabai Merah Keriting', 'Cabai Rawit', 'Cabai Rawit Hijau', 'Cabai Rawit Merah', 'Minyak Goreng', 'Minyak Goreng Curah', 'Minyak Goreng Kemasan Bermerk 1', 'Minyak Goreng Kemasan Bermerk 2'
'Gula Pasir', 'Gula Pasir Kualitas Premium', 'Gula Pasir Lokal'

LOKASI: 'Jakarta Pusat', 'Kota Bogor', 'Kota Depok', 'Kota Tangerang', 'Kota Bekasi'

PENGGUNAAN DI KOLOM KUNCI: 'Lokasi_Komoditas'
"""

kolom_kunci = [
    'Jakarta Pusat_Cabai Merah Keriting',
    'Kota Bogor_Cabai Merah Keriting',
    'Jakarta Pusat_Cabai Rawit Merah',
    'Jakarta Pusat_Bawang Merah',
    'Jakarta Pusat_Beras Kualitas Medium I'
    # Tambahkan atau ganti sesuai kebutuhan
]

# 4. Konfigurasi untuk Heatmap Korelasi
#    Jika Anda punya sangat banyak kolom (>50), mungkin lebih baik pilih subset
#    Jika kolom_kunci_corr kosong, akan coba hitung untuk semua kolom (bisa lambat)
kolom_kunci_corr = kolom_kunci # Gunakan kolom kunci yang sama untuk korelasi awal
# Atau pilih subset lain: kolom_kunci_corr = ['Col1', 'Col2', ...]
# Atau biarkan kosong: kolom_kunci_corr = []

# --- Memuat Data Wide ---
df_wide = None
print("Mencoba memuat data wide yang sudah diimputasi...")
if os.path.exists(wide_parquet_path):
    try:
        print(f"Memuat dari Parquet: {wide_parquet_path}")
        df_wide = pd.read_parquet(wide_parquet_path)
        # Pastikan index adalah DatetimeIndex
        if not isinstance(df_wide.index, pd.DatetimeIndex):
             df_wide.index = pd.to_datetime(df_wide.index)
        print("Berhasil memuat dari Parquet.")
    except Exception as e:
        print(f"Gagal memuat Parquet: {e}. Mencoba CSV...")
        df_wide = None

if df_wide is None and os.path.exists(wide_csv_path):
    try:
        print(f"Memuat dari CSV: {wide_csv_path}")
        # Penting: Tentukan kolom Tanggal sebagai index saat membaca CSV
        df_wide = pd.read_csv(wide_csv_path, index_col='Tanggal', parse_dates=True)
        print("Berhasil memuat dari CSV.")
    except Exception as e:
        print(f"Gagal memuat CSV: {e}")
        df_wide = None

if df_wide is None:
    print("\nError: Tidak dapat memuat data wide gabungan. Pastikan file .parquet atau .csv ada.")
else:
    print("\nData wide berhasil dimuat. Info:")
    print(df_wide.info())
    print("\nMemulai EDA pada data wide...")

    # --- 1. Visualisasi Beberapa Deret Waktu Kunci ---
    print("\n1. Memvisualisasikan beberapa deret waktu kunci...")
    # Cek apakah kolom kunci ada di DataFrame
    kolom_tersedia_untuk_plot = [col for col in kolom_kunci if col in df_wide.columns]
    if not kolom_tersedia_untuk_plot:
        print("  Peringatan: Tidak ada kolom kunci yang ditentukan ditemukan dalam data wide.")
    else:
        print(f"  Memplot kolom: {kolom_tersedia_untuk_plot}")
        fig1 = px.line(df_wide[kolom_tersedia_untuk_plot],
                      title='Visualisasi Tren Harga Beberapa Komoditas-Lokasi Kunci',
                      labels={'value': 'Harga (Rp)', 'variable': 'Komoditas-Lokasi', 'index': 'Tanggal'})
        fig1.update_layout(legend_title_text='Komoditas-Lokasi')
        fig1.show()

    # --- 2. Analisis Korelasi ---
    print("\n2. Menghitung & Memvisualisasikan Matriks Korelasi...")
    if not kolom_kunci_corr:
         print("  Menghitung korelasi untuk SEMUA kolom (mungkin lambat)...")
         df_corr_subset = df_wide
    else:
         # Cek kolom korelasi yang tersedia
         kolom_tersedia_untuk_corr = [col for col in kolom_kunci_corr if col in df_wide.columns]
         if len(kolom_tersedia_untuk_corr) < 2:
              print("  Peringatan: Kurang dari 2 kolom kunci korelasi ditemukan/ditentukan. Tidak dapat menghitung korelasi.")
              df_corr_subset = None
         else:
              print(f"  Menghitung korelasi untuk kolom: {kolom_tersedia_untuk_corr}")
              df_corr_subset = df_wide[kolom_tersedia_untuk_corr]

    if df_corr_subset is not None:
         correlation_matrix = df_corr_subset.corr()
         print("\nMatriks Korelasi (Subset):")
         print(correlation_matrix)

         # Visualisasi Heatmap
         plt.figure(figsize=(10, 8)) # Sesuaikan ukuran jika perlu
         sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
         plt.title('Heatmap Korelasi Harga Antar Komoditas-Lokasi Kunci')
         plt.xticks(rotation=45, ha='right')
         plt.yticks(rotation=0)
         plt.tight_layout() # Agar label tidak tumpang tindih
         plt.show()
    else:
         print("  Perhitungan korelasi dilewati.")


    # --- 3. (Opsional) Uji Stasioneritas (ADF Test) ---
    print("\n3. (Opsional) Melakukan Uji Stasioneritas (ADF Test) pada kolom kunci...")

    def test_stationarity(timeseries, col_name=""):
        """Melakukan tes ADF dan mencetak hasilnya."""
        print(f"\n--- Hasil Augmented Dickey-Fuller Test untuk: {col_name} ---")
        # Penting: hilangkan NaN SEBELUM tes ADF jika masih ada (seharusnya tidak jika imputasi berhasil)
        timeseries_clean = timeseries.dropna()
        if timeseries_clean.empty:
            print("  Data kolom kosong setelah dropna, tidak bisa melakukan tes.")
            return

        result = adfuller(timeseries_clean, autolag='AIC')
        print(f'ADF Statistic: {result[0]:.3f}')
        print(f'p-value: {result[1]:.3f}')
        print('Critical Values:')
        for key, value in result[4].items():
            print(f'\t{key}: {value:.3f}')

        # Interpretasi hasil
        if result[1] <= 0.05:
            print("=> Kesimpulan: Data kemungkinan stasioner (Tolak hipotesis nol)")
        else:
            print("=> Kesimpulan: Data kemungkinan non-stasioner (Gagal menolak hipotesis nol)")

    # Terapkan tes pada kolom kunci yang tersedia
    if kolom_tersedia_untuk_plot: # Gunakan kolom yang sama dengan plot awal
         print(f"  Menguji stasioneritas untuk kolom: {kolom_tersedia_untuk_plot}")
         for col in kolom_tersedia_untuk_plot:
              test_stationarity(df_wide[col], col_name=col)
    else:
         print("  Tidak ada kolom kunci untuk diuji stasioneritasnya.")


    print("\n--- EDA pada data wide selesai ---")
    print("Gunakan hasil korelasi dan visualisasi untuk membantu memilih fitur input (X) dan target (y) untuk model LSTM.")

"""# **==== DATA SPLIT & MODEL TRAINING ====**"""

import pandas as pd
import numpy as np
import os

# --- Konfigurasi ---
# 1. Path ke folder tempat file WIDE gabungan disimpan
folder_path = r'/content/drive/MyDrive/project02' # Sesuaikan

# 2. Nama file Parquet/CSV WIDE yang sudah diimputasi
parquet_file_wide = 'data_pangan_jabodetabek_wide_imputed.parquet'
csv_file_wide = 'data_pangan_jabodetabek_wide_imputed.csv'

wide_parquet_path = os.path.join(folder_path, parquet_file_wide)
wide_csv_path = os.path.join(folder_path, csv_file_wide)

# 3. Definisi Kolom Target dan Fitur Input (SESUAIKAN JIKA PERLU)
target_column = 'Jakarta Pusat_Beras Kualitas Medium I'
feature_columns = [
    'Jakarta Pusat_Beras Kualitas Medium I', # Tetap sertakan target sebagai fitur input
    'Kota Bogor_Beras Kualitas Medium I',
    'Kota Bekasi_Beras Kualitas Medium I',
    'Kota Depok_Beras Kualitas Medium I',
    'Kota Tangerang_Beras Kualitas Medium I'
]

# 4. Proporsi Pembagian Data
train_split_prop = 0.70 # 70% untuk training
val_split_prop = 0.15   # 15% untuk validasi
# Sisa (1 - 0.70 - 0.15 = 0.15) otomatis menjadi test set (15%)

# --- Memuat Data Wide ---
df_wide = None
print("Mencoba memuat data wide yang sudah diimputasi...")
# (Kode pemuatan data sama seperti sebelumnya - prioritaskan Parquet)
if os.path.exists(wide_parquet_path):
    try:
        print(f"Memuat dari Parquet: {wide_parquet_path}")
        df_wide = pd.read_parquet(wide_parquet_path)
        if not isinstance(df_wide.index, pd.DatetimeIndex):
             df_wide.index = pd.to_datetime(df_wide.index)
        print("Berhasil memuat dari Parquet.")
    except Exception as e:
        print(f"Gagal memuat Parquet: {e}. Mencoba CSV...")
        df_wide = None

if df_wide is None and os.path.exists(wide_csv_path):
    try:
        print(f"Memuat dari CSV: {wide_csv_path}")
        df_wide = pd.read_csv(wide_csv_path, index_col='Tanggal', parse_dates=True)
        print("Berhasil memuat dari CSV.")
    except Exception as e:
        print(f"Gagal memuat CSV: {e}")
        df_wide = None

if df_wide is None:
    print("\nError: Tidak dapat memuat data wide gabungan.")
else:
    print("\nData wide berhasil dimuat.")

    # --- Langkah 4.1: Pemilihan Fitur Input (X) dan Target (y) ---
    print("\nLangkah 4.1: Memilih Fitur Input (X) dan Target (y)...")

    # Pastikan semua kolom yang dibutuhkan ada
    all_needed_columns = list(set([target_column] + feature_columns))
    missing_cols = [col for col in all_needed_columns if col not in df_wide.columns]

    if missing_cols:
        print(f"\nError: Kolom berikut tidak ditemukan di DataFrame: {missing_cols}")
        print("Pastikan nama kolom di 'target_column' dan 'feature_columns' sudah benar.")
    else:
        print(f"  Target (y): '{target_column}'")
        print(f"  Fitur Input (X): {feature_columns}")

        # Membuat DataFrame X dan Series y
        X = df_wide[feature_columns].copy()
        y = df_wide[target_column].copy()

        # --- Langkah 4.2: Pembagian Data (Train/Validation/Test Split) ---
        print("\nLangkah 4.2: Melakukan Pembagian Data secara Kronologis...")

        n_total = len(X)
        n_train = int(n_total * train_split_prop)
        n_val = int(n_total * val_split_prop)
        # n_test = n_total - n_train - n_val # Sisa data

        # Melakukan slicing berdasarkan posisi integer (iloc) karena index sudah urut waktu
        X_train = X.iloc[:n_train]
        y_train = y.iloc[:n_train]

        X_val = X.iloc[n_train : n_train + n_val]
        y_val = y.iloc[n_train : n_train + n_val]

        X_test = X.iloc[n_train + n_val:]
        y_test = y.iloc[n_train + n_val:]

        # --- Verifikasi Hasil Split ---
        print("\nVerifikasi Hasil Pembagian Data:")
        print(f"  Total data     : {n_total} baris")
        print(f"  Training Set   : {len(X_train)} baris ({len(X_train)/n_total:.1%}) | Tanggal: {X_train.index.min().strftime('%Y-%m-%d')} s/d {X_train.index.max().strftime('%Y-%m-%d')}")
        print(f"  Validation Set : {len(X_val)} baris ({len(X_val)/n_total:.1%}) | Tanggal: {X_val.index.min().strftime('%Y-%m-%d')} s/d {X_val.index.max().strftime('%Y-%m-%d')}")
        print(f"  Test Set       : {len(X_test)} baris ({len(X_test)/n_total:.1%}) | Tanggal: {X_test.index.min().strftime('%Y-%m-%d')} s/d {X_test.index.max().strftime('%Y-%m-%d')}")

        # Tampilkan shape untuk memastikan dimensi benar
        print("\nDimensi (Shape) Data Hasil Split:")
        print(f"  X_train: {X_train.shape}, y_train: {y_train.shape}")
        print(f"  X_val:   {X_val.shape}, y_val:   {y_val.shape}")
        print(f"  X_test:  {X_test.shape}, y_test:  {y_test.shape}")

        print("\nData siap untuk tahap Scaling dan Sequencing.")

# Anda sekarang memiliki: X_train, y_train, X_val, y_val, X_test, y_test
# Pastikan untuk menyimpan variabel-variabel ini jika Anda menjalankan di notebook
# atau muat kembali data wide dan jalankan kode ini lagi sebelum lanjut ke scaling.

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler # Import scaler
import os

# --- Pastikan Variabel Hasil Split Tersedia ---
# Jika Anda menjalankan ini di sel/skrip terpisah, muat ulang data wide
# dan jalankan lagi kode splitting dari langkah sebelumnya.
# Kode ini berasumsi X_train, y_train, dkk. sudah ada.
if 'X_train' not in locals() or 'y_train' not in locals():
     print("Error: Variabel X_train/y_train dkk. tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Data Splitting sebelumnya.")
     # Berhenti jika variabel tidak ada
     # exit() # Uncomment jika menjalankan sebagai skrip mandiri

else:
    print("Memulai Langkah 4.3: Scaling Data...")

    # --- Membuat Scaler ---
    # 1. Scaler untuk Fitur Input (X)
    x_scaler = MinMaxScaler(feature_range=(0, 1)) # Skala ke rentang 0-1

    # 2. Scaler untuk Target (y) - Scaler terpisah penting untuk inverse transform
    y_scaler = MinMaxScaler(feature_range=(0, 1))

    # --- Scaling Data ---
    # PENTING: Fit HANYA pada data training!

    # 1. Scale X_train (Fit dan Transform)
    print("  Scaling X_train...")
    # fit_transform mengembalikan NumPy array
    X_train_scaled = x_scaler.fit_transform(X_train)
    print(f"    Shape X_train_scaled: {X_train_scaled.shape}")

    # 2. Scale y_train (Fit dan Transform)
    print("  Scaling y_train...")
    # y_train adalah Series, perlu di-reshape jadi 2D untuk scaler
    # .values mengubah ke NumPy array, .reshape(-1, 1) menjadikannya kolom tunggal
    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))
    print(f"    Shape y_train_scaled: {y_train_scaled.shape}")

    # 3. Scale X_val (Hanya Transform, menggunakan x_scaler yang sudah di-fit)
    print("  Scaling X_val...")
    X_val_scaled = x_scaler.transform(X_val)
    print(f"    Shape X_val_scaled: {X_val_scaled.shape}")

    # 4. Scale y_val (Hanya Transform, menggunakan y_scaler yang sudah di-fit)
    print("  Scaling y_val...")
    y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1))
    print(f"    Shape y_val_scaled: {y_val_scaled.shape}")

    # 5. Scale X_test (Hanya Transform, menggunakan x_scaler yang sudah di-fit)
    print("  Scaling X_test...")
    X_test_scaled = x_scaler.transform(X_test)
    print(f"    Shape X_test_scaled: {X_test_scaled.shape}")

    # 6. Scale y_test (Hanya Transform, menggunakan y_scaler yang sudah di-fit)
    print("  Scaling y_test...")
    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))
    print(f"    Shape y_test_scaled: {y_test_scaled.shape}")

    # --- Verifikasi Scaling (Opsional) ---
    print("\nVerifikasi Scaling (Contoh X_train_scaled):")
    print(f"  Min value: {np.min(X_train_scaled):.4f}")
    print(f"  Max value: {np.max(X_train_scaled):.4f}")
    # Cetak min/max per kolom jika ingin lebih detail
    print(f"  Min per kolom: {np.min(X_train_scaled, axis=0)}")
    print(f"  Max per kolom: {np.max(X_train_scaled, axis=0)}")

    print("\nScaling data selesai.")
    print("Variabel hasil: X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_test_scaled")
    print("PENTING: Simpan objek 'x_scaler' dan 'y_scaler' jika Anda perlu memuat model nanti!")
    # Contoh menyimpan scaler (jika diperlukan):
    import joblib
    joblib.dump(x_scaler, os.path.join(folder_path, 'x_scaler.gz'))
    joblib.dump(y_scaler, os.path.join(folder_path, 'y_scaler.gz'))

import numpy as np
import pandas as pd # Hanya untuk verifikasi jika perlu

# --- Konfigurasi Windowing ---
# Tentukan jumlah langkah waktu sebelumnya (hari) yang akan digunakan
# untuk memprediksi hari berikutnya.
# Nilai umum bisa 30, 60, 90, dll. Perlu eksperimen.
look_back = 60 # Contoh: menggunakan 60 hari terakhir untuk prediksi hari ke-61

# --- Pastikan Variabel Hasil Scaling Tersedia ---
# Asumsi X_train_scaled, y_train_scaled, dkk. sudah ada dari langkah sebelumnya
if 'X_train_scaled' not in locals() or 'y_train_scaled' not in locals():
     print("Error: Variabel ..._scaled tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Scaling sebelumnya.")
     # exit() # Uncomment jika perlu

else:
    print(f"Memulai Langkah 4.4: Pembuatan Sequence (Windowing) dengan look_back={look_back}...")

    # --- Fungsi untuk Membuat Sequence ---
    def create_sequences(data_x, data_y, time_steps=1):
        """
        Mengubah data time series menjadi format sequence untuk LSTM.
        Args:
            data_x (np.array): Array fitur input (scaled). Shape: [n_samples, n_features]
            data_y (np.array): Array target (scaled). Shape: [n_samples, 1]
            time_steps (int): Jumlah langkah waktu sebelumnya (look_back).
        Returns:
            np.array: Array X dalam format sequence [n_sequences, time_steps, n_features]
            np.array: Array y dalam format sequence [n_sequences, 1]
        """
        output_x = []
        output_y = []
        # Iterasi data, berhenti 'time_steps' sebelum akhir agar target y selalu ada
        for i in range(len(data_x) - time_steps):
            # Ambil sequence input X sepanjang 'time_steps'
            sequence_x = data_x[i:(i + time_steps)]
            output_x.append(sequence_x)

            # Ambil target y pada langkah waktu setelah sequence X
            target_y = data_y[i + time_steps]
            output_y.append(target_y)

        return np.array(output_x), np.array(output_y)

    # --- Membuat Sequence untuk Setiap Set Data ---

    print("  Membuat sequence untuk Training Set...")
    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, look_back)
    print(f"    Shape X_train_seq: {X_train_seq.shape}")
    print(f"    Shape y_train_seq: {y_train_seq.shape}")

    print("  Membuat sequence untuk Validation Set...")
    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, look_back)
    print(f"    Shape X_val_seq: {X_val_seq.shape}")
    print(f"    Shape y_val_seq: {y_val_seq.shape}")

    print("  Membuat sequence untuk Test Set...")
    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, look_back)
    print(f"    Shape X_test_seq: {X_test_seq.shape}")
    print(f"    Shape y_test_seq: {y_test_seq.shape}")

    # --- Verifikasi (Opsional) ---
    # Cek apakah jumlah sequence masuk akal (original_length - look_back)
    expected_train_seq_len = len(X_train_scaled) - look_back
    if X_train_seq.shape[0] == expected_train_seq_len:
        print("\n  Verifikasi panjang sequence training: OK")
    else:
        print(f"\n  Peringatan: Panjang sequence training ({X_train_seq.shape[0]}) tidak sesuai harapan ({expected_train_seq_len}).")

    print("\nPembuatan sequence selesai.")
    print("Variabel hasil: X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq")
    print("Data siap untuk dimasukkan ke model LSTM.")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
import os

# --- Konfigurasi Model ---
# Kita perlu tahu bentuk input dari data sequence yang sudah dibuat
# Asumsi variabel ini sudah ada dari langkah windowing:
# X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq

# Ambil dimensi dari X_train_seq (jika sudah dibuat)
# Jika belum, Anda perlu menjalankannya atau mendefinisikan look_back dan n_features secara manual
try:
    # Shape: [jumlah_sampel, timesteps/look_back, jumlah_fitur]
    input_timesteps = X_train_seq.shape[1]
    input_features = X_train_seq.shape[2]
    print(f"Input shape terdeteksi: timesteps={input_timesteps}, features={input_features}")
except NameError:
    print("Error: Variabel X_train_seq tidak ditemukan. Menggunakan nilai default (harap sesuaikan!).")
    # --> GANTI INI JIKA PERLU <--
    look_back_default = 60 # Harus sama dengan look_back saat membuat sequence
    n_features_default = 5 # Harus sama dengan jumlah kolom di feature_columns
    input_timesteps = look_back_default
    input_features = n_features_default
    print(f"Menggunakan nilai default: timesteps={input_timesteps}, features={input_features}")


# Hyperparameter Model Awal (bisa di-tuning nanti)
lstm_units_1 = 50   # Jumlah neuron di layer LSTM pertama
dropout_rate = 0.2  # Rate untuk dropout layer (mencegah overfitting)
# lstm_units_2 = 50 # Uncomment jika ingin menambah layer LSTM kedua
optimizer = 'adam'      # Algoritma optimasi
loss_function = 'mean_squared_error' # Fungsi loss untuk regresi
metrics_to_monitor = ['mean_absolute_error'] # Metrik tambahan untuk dipantau

# --- Membangun Arsitektur Model ---
print("\nMembangun arsitektur model LSTM...")

model = Sequential(name="LSTM_Multivariate_HargaPangan")

# Input Layer (implisit dari layer pertama) + Layer LSTM Pertama
# input_shape = (jumlah langkah waktu, jumlah fitur per langkah waktu)
model.add(Input(shape=(input_timesteps, input_features), name='Input_Layer')) # Cara eksplisit mendefinisikan input
model.add(LSTM(units=lstm_units_1,
               # return_sequences=True, # Set True jika ada layer LSTM lain setelah ini
               name='LSTM_Layer_1'))

# Layer Dropout
model.add(Dropout(dropout_rate, name='Dropout_Layer_1'))

# Layer Output Dense
# units=1 karena kita memprediksi satu nilai (harga target)
# activation='linear' (default) cocok untuk regresi
model.add(Dense(units=1, name='Output_Layer'))

# --- Kompilasi Model ---
print(f"Mengompilasi model dengan optimizer='{optimizer}' dan loss='{loss_function}'...")
model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics_to_monitor)

# --- Tampilkan Ringkasan Model ---
print("\nRingkasan Arsitektur Model:")
model.summary()

print("\nModel LSTM berhasil dibangun dan dikompilasi.")
print("Model siap untuk dilatih menggunakan model.fit()")

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt
from plotly.subplots import make_subplots
import os
import time # Untuk mengukur waktu training

# --- Konfigurasi Training ---
# Hyperparameter training (bisa di-tuning)
epochs = 50        # Jumlah maksimum iterasi training (EarlyStopping bisa menghentikan lebih awal)
batch_size = 32    # Jumlah sampel yang diproses sebelum bobot diperbarui

# Konfigurasi Callbacks
# 1. Early Stopping: Menghentikan training jika loss validasi tidak membaik
early_stopping_patience = 10 # Berapa epoch menunggu perbaikan sebelum berhenti
early_stopping_monitor = 'val_loss' # Pantau loss di set validasi

# 2. Model Checkpoint: Menyimpan model terbaik (berdasarkan val_loss)
#    Tentukan path folder dan nama file untuk menyimpan model terbaik
model_checkpoint_filepath = os.path.join(folder_path, 'lstm_multivariate_best_model.keras') # Gunakan .keras
model_checkpoint_monitor = 'val_loss'

# 3. Reduce Learning Rate on Plateau (Opsional tapi sering membantu)
#    Mengurangi learning rate jika loss validasi stagnan
reduce_lr_factor = 0.2   # Faktor pengurangan LR (new_lr = lr * factor)
reduce_lr_patience = 5   # Berapa epoch menunggu sebelum mengurangi LR
reduce_lr_monitor = 'val_loss'

# --- Pastikan Variabel Sequence Tersedia ---
# Asumsi X_train_seq, y_train_seq, X_val_seq, y_val_seq sudah ada
if 'X_train_seq' not in locals() or 'y_train_seq' not in locals():
    print("Error: Variabel ..._seq tidak ditemukan.")
    print("Pastikan Anda sudah menjalankan kode Windowing sebelumnya.")
    # exit()
elif 'model' not in locals():
    print("Error: Variabel 'model' tidak ditemukan.")
    print("Pastikan Anda sudah menjalankan kode Membangun & Kompilasi Model sebelumnya.")
    # exit()
else:
    print("Memulai Langkah Training Model...")

    # --- Menyiapkan Callbacks ---
    early_stopping = EarlyStopping(monitor=early_stopping_monitor,
                                 patience=early_stopping_patience,
                                 verbose=1, # Cetak pesan saat berhenti
                                 restore_best_weights=True) # Kembalikan bobot terbaik saat berhenti

    model_checkpoint = ModelCheckpoint(filepath=model_checkpoint_filepath,
                                       monitor=model_checkpoint_monitor,
                                       save_best_only=True, # Hanya simpan yang terbaik
                                       verbose=1) # Cetak pesan saat menyimpan model

    reduce_lr = ReduceLROnPlateau(monitor=reduce_lr_monitor,
                                factor=reduce_lr_factor,
                                patience=reduce_lr_patience,
                                verbose=1, # Cetak pesan saat LR dikurangi
                                min_lr=1e-6) # Learning rate minimum

    callbacks_list = [early_stopping, model_checkpoint, reduce_lr]

    # --- Melatih Model ---
    print("\nMelatih model...")
    start_time = time.time()

    history = model.fit(X_train_seq, y_train_seq,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(X_val_seq, y_val_seq),
                        callbacks=callbacks_list,
                        verbose=1) # verbose=1 untuk progress bar, verbose=2 untuk satu baris per epoch

    end_time = time.time()
    training_time = end_time - start_time
    print(f"\nTraining selesai dalam {training_time:.2f} detik.")

    # --- Visualisasi Hasil Training ---
    print("\nMembuat plot histori training (Loss & Metrik)...")

    history_df = pd.DataFrame(history.history)

    fig = make_subplots(rows=1, cols=2, subplot_titles=('Model Loss (MSE)', f'Model Metric ({metrics_to_monitor[0]})'))

    # Plot Loss
    fig.add_trace(go.Scatter(y=history_df['loss'], name='Training Loss', mode='lines'), row=1, col=1)
    fig.add_trace(go.Scatter(y=history_df['val_loss'], name='Validation Loss', mode='lines'), row=1, col=1)

    # Plot Metrik (misal, MAE)
    metric_name = metrics_to_monitor[0] # Ambil nama metrik pertama yang dimonitor
    if metric_name in history_df.columns:
         fig.add_trace(go.Scatter(y=history_df[metric_name], name=f'Training {metric_name}', mode='lines'), row=1, col=2)
         fig.add_trace(go.Scatter(y=history_df[f'val_{metric_name}'], name=f'Validation {metric_name}', mode='lines'), row=1, col=2)
    else:
         print(f"Peringatan: Metrik '{metric_name}' tidak ditemukan dalam histori.")


    fig.update_layout(title_text='Histori Training Model LSTM', height=400)
    fig.update_xaxes(title_text='Epoch', row=1, col=1)
    fig.update_xaxes(title_text='Epoch', row=1, col=2)
    fig.update_yaxes(title_text='Loss (MSE)', row=1, col=1)
    fig.update_yaxes(title_text=f'Metric ({metrics_to_monitor[0]})', row=1, col=2)
    fig.show()

    print("\nModel terbaik disimpan di:", model_checkpoint_filepath)
    print("Langkah selanjutnya: Evaluasi model pada Test Set.")

    # Simpan history untuk referensi (opsional)
    # history_df.to_csv(os.path.join(folder_path, 'training_history.csv'))

"""# **==== EVALUASI & HASIL PREDIKSI MODEL ====**"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import plotly.graph_objects as go
import os

# --- Pastikan Variabel/Objek Tersedia ---
# Asumsi variabel/objek ini sudah ada dari langkah-langkah sebelumnya:
# - X_test_seq: Data fitur test (scaled, sequenced)
# - y_test_seq: Data target test (scaled, sequenced) - BENTUK ASLI y_test_seq PENTING
# - y_scaler: Objek MinMaxScaler yang sudah di-fit pada y_train
# - model_checkpoint_filepath: Path ke file .keras model terbaik
# - y_test: Series PANDAS target test ASLI (unscaled, unsequenced) - untuk plotting & perbandingan
# - look_back: Jumlah timestep windowing
# - target_column: Nama kolom target

if 'X_test_seq' not in locals() or 'y_test_seq' not in locals():
    print("Error: Variabel X_test_seq/y_test_seq tidak ditemukan.")
    print("Pastikan Anda sudah menjalankan kode Windowing sebelumnya.")
    # exit()
elif 'y_scaler' not in locals():
     print("Error: Variabel 'y_scaler' tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Scaling sebelumnya.")
     # exit()
elif 'model_checkpoint_filepath' not in locals() or not os.path.exists(model_checkpoint_filepath):
     print(f"Error: File model terbaik tidak ditemukan di {model_checkpoint_filepath}")
     print("Pastikan path sudah benar dan model sudah disimpan saat training.")
     # exit()
elif 'y_test' not in locals():
     print(f"Error: Variabel y_test (data asli unscaled) tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Data Splitting.")
     # exit()

else:
    print("Memulai Evaluasi Model pada Test Set...")

    # --- 1. Muat Model Terbaik ---
    print(f"Memuat model terbaik dari: {model_checkpoint_filepath}")
    try:
        best_model = tf.keras.models.load_model(model_checkpoint_filepath)
        print("Model berhasil dimuat.")
        best_model.summary() # Tampilkan summary untuk konfirmasi
    except Exception as e:
        print(f"Error saat memuat model: {e}")
        # exit()
        best_model = None # Set ke None jika gagal

    if best_model:
        # --- 2. Lakukan Prediksi pada Test Set ---
        print("\nMelakukan prediksi pada X_test_seq...")
        y_pred_scaled = best_model.predict(X_test_seq)
        print(f"Shape hasil prediksi (scaled): {y_pred_scaled.shape}")

        # --- 3. Inverse Transform Prediksi ---
        print("Melakukan inverse transform pada hasil prediksi...")
        # y_pred_scaled berbentuk [n_samples, 1], cocok untuk scaler
        y_pred_actual = y_scaler.inverse_transform(y_pred_scaled)
        print(f"Shape hasil prediksi (actual scale): {y_pred_actual.shape}")
        # Ubah ke 1D array untuk perhitungan metrik yang lebih mudah
        y_pred_actual_1d = y_pred_actual.flatten()

        # --- 4. Inverse Transform Nilai y_test_seq (Target Sebenarnya) ---
        print("Melakukan inverse transform pada nilai y_test_seq (target sebenarnya)...")
        # y_test_seq mungkin berbentuk [n_samples, 1] jika hasil dari create_sequences
        # Jika tidak, reshape dulu
        if y_test_seq.ndim == 1:
             y_test_seq_reshaped = y_test_seq.reshape(-1, 1)
        else:
             y_test_seq_reshaped = y_test_seq

        y_test_actual = y_scaler.inverse_transform(y_test_seq_reshaped)
        print(f"Shape target test (actual scale): {y_test_actual.shape}")
        # Ubah ke 1D array
        y_test_actual_1d = y_test_actual.flatten()

        # --- 5. Hitung Metrik Evaluasi ---
        print("\nMenghitung Metrik Evaluasi (pada skala asli):")
        mae = mean_absolute_error(y_test_actual_1d, y_pred_actual_1d)
        rmse = np.sqrt(mean_squared_error(y_test_actual_1d, y_pred_actual_1d))
        mape = mean_absolute_percentage_error(y_test_actual_1d, y_pred_actual_1d) * 100 # Kali 100 untuk persentase

        print(f"  Mean Absolute Error (MAE): {mae:,.2f} (Rata-rata error absolut dalam Rupiah)")
        print(f"  Root Mean Squared Error (RMSE): {rmse:,.2f} (Akar rata-rata kuadrat error dalam Rupiah)")
        print(f"  Mean Absolute Percentage Error (MAPE): {mape:.2f}% (Rata-rata persentase error)")

        # --- 6. Visualisasi Prediksi vs Aktual ---
        print("\nMembuat plot perbandingan Prediksi vs Aktual pada Test Set...")

        # Kita butuh tanggal yang sesuai dengan y_test_actual_1d dan y_pred_actual_1d
        # Panjang y_test_actual_1d adalah len(y_test) - look_back
        # Jadi, kita ambil tanggal dari y_test ASLI (unscaled) mulai dari index ke 'look_back'
        if len(y_test_actual_1d) == len(y_test) - look_back:
            dates_for_plot = y_test.index[look_back:]

            # Membuat DataFrame untuk plotting
            plot_df = pd.DataFrame({
                'Tanggal': dates_for_plot,
                'Harga Aktual': y_test_actual_1d,
                'Harga Prediksi': y_pred_actual_1d
            })

            fig_eval = go.Figure()
            fig_eval.add_trace(go.Scatter(x=plot_df['Tanggal'], y=plot_df['Harga Aktual'],
                                        mode='lines', name='Harga Aktual', line=dict(color='blue')))
            fig_eval.add_trace(go.Scatter(x=plot_df['Tanggal'], y=plot_df['Harga Prediksi'],
                                        mode='lines', name='Harga Prediksi LSTM', line=dict(color='red', dash='dash')))

            fig_eval.update_layout(
                title=f'Perbandingan Harga Aktual vs Prediksi LSTM - Test Set<br>({target_column})',
                xaxis_title='Tanggal',
                yaxis_title='Harga (Rp)',
                hovermode='x unified'
            )
            fig_eval.show()
            print("Plot evaluasi ditampilkan.")

        else:
             print("Peringatan: Panjang data prediksi dan target test tidak cocok dengan y_test asli setelah look_back. Tidak dapat membuat plot tanggal yang benar.")


    print("\n--- Evaluasi Model Selesai ---")

import numpy as np
import pandas as pd # Hanya untuk memastikan tipe data jika perlu
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import os

# --- Pastikan Variabel Hasil Prediksi & Inverse Transform Tersedia ---
# Asumsi variabel ini sudah ada dari langkah evaluasi sebelumnya:
# - y_test_actual_1d: Target test asli (unscaled, 1D NumPy array)
# - y_pred_actual_1d: Prediksi model (unscaled, 1D NumPy array)
# - target_column: Nama kolom target (untuk konteks)

if 'y_test_actual_1d' not in locals() or 'y_pred_actual_1d' not in locals():
     print("Error: Variabel y_test_actual_1d / y_pred_actual_1d tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Evaluasi Model sebelumnya yang menghasilkan variabel ini.")
     # exit() # Uncomment jika perlu
elif 'target_column' not in locals():
     # Tetapkan nama default jika belum ada, tapi idealnya ada dari langkah sebelumnya
     target_column = "Target Tidak Diketahui"
     print("Peringatan: Nama kolom target tidak ditemukan, menggunakan default.")

else:
    print(f"Menghitung Metrik Kuantitatif untuk Target: '{target_column}'")

    # Pastikan kedua array memiliki panjang yang sama
    if len(y_test_actual_1d) != len(y_pred_actual_1d):
        print(f"Error: Panjang y_test_actual_1d ({len(y_test_actual_1d)}) dan y_pred_actual_1d ({len(y_pred_actual_1d)}) tidak sama!")
    else:
        # --- Hitung Metrik ---
        mae = mean_absolute_error(y_test_actual_1d, y_pred_actual_1d)
        mse = mean_squared_error(y_test_actual_1d, y_pred_actual_1d)
        rmse = np.sqrt(mse) # Akar dari MSE
        # Hati-hati dengan MAPE jika y_test_actual_1d ada yang nol (tidak mungkin untuk harga)
        mape = mean_absolute_percentage_error(y_test_actual_1d, y_pred_actual_1d) * 100 # Jadikan persentase

        # --- Cetak Hasil ---
        print("\n--- Hasil Metrik Evaluasi (Test Set) ---")
        print(f"1. Mean Absolute Error (MAE):  {mae:,.2f} Rupiah")
        print(f"   -> Rata-rata, prediksi model meleset sebesar nilai ini dari harga aktual.")
        print(f"2. Root Mean Squared Error (RMSE): {rmse:,.2f} Rupiah")
        print(f"   -> Mirip MAE, tapi memberi bobot lebih besar pada error yang jauh (outlier). Biasanya > MAE.")
        print(f"3. Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
        print(f"   -> Rata-rata, prediksi model meleset sebesar persentase ini dari harga aktual.")
        print("------------------------------------------")

        # --- Penjelasan Hasil (Ini bagian penting!) ---
        print("\n--- Penjelasan Metrik ---")
        print(f"*   MAE ({mae:,.2f} Rp): Secara rata-rata, prediksi harga '{target_column}' Anda meleset sekitar {mae:,.0f} Rupiah dari harga sebenarnya pada periode test.")
        print(f"*   RMSE ({rmse:,.2f} Rp): Ini adalah akar dari rata-rata kuadrat error. Nilainya lebih tinggi dari MAE ({rmse > mae}) menunjukkan adanya beberapa prediksi yang errornya cukup besar, karena RMSE lebih sensitif terhadap error besar.")
        print(f"*   MAPE ({mape:.2f}%): Secara rata-rata, error prediksi Anda adalah sekitar {mape:.2f}% dari nilai harga aktual. Metrik ini berguna untuk perbandingan karena tidak terikat unit Rupiah.")

        print("\n--- Bagaimana Menginterpretasikan 'Baik' atau 'Buruk'? ---")
        print("*   Relatif terhadap Harga Komoditas: MAE 500 Rupiah mungkin sangat bagus untuk Daging Sapi (harga puluhan/ratusan ribu) tapi mungkin kurang bagus untuk Bawang Merah (harga puluhan ribu). Lihat MAPE untuk perbandingan relatif.")
        print("*   Relatif terhadap Baseline/Model Lain: Bandingkan metrik ini dengan prediksi Naive (harga besok = harga hari ini) atau model lain seperti Prophet/ARIMA. Apakah LSTM memberikan peningkatan signifikan?")
        print("*   Relatif terhadap Kebutuhan: Seberapa akurat prediksi yang Anda butuhkan untuk tujuan akhir Anda? Apakah error rata-rata sekian Rupiah atau sekian persen dapat diterima?")
        print("*   Lihat Kembali Plot: Plot visual membantu memahami *di mana* error terbesar terjadi (misal saat lonjakan harga). Metrik hanya memberikan nilai rata-rata.")

        print("\nLangkah selanjutnya bisa berupa tuning hyperparameter, mencoba fitur lain, atau membandingkan dengan model lain jika hasil ini belum memuaskan.")

"""# **==== BATAS SUCI (PERBANDINGAN) ====**"""

import pandas as pd
import numpy as np
import os

# --- Konfigurasi ---
# 1. Path ke folder tempat file WIDE gabungan disimpan
folder_path = r'/content/drive/MyDrive/project02' # Sesuaikan

# 2. Nama file Parquet/CSV WIDE yang sudah diimputasi
parquet_file_wide = 'data_pangan_jabodetabek_wide_imputed.parquet'
csv_file_wide = 'data_pangan_jabodetabek_wide_imputed.csv'

wide_parquet_path = os.path.join(folder_path, parquet_file_wide)
wide_csv_path = os.path.join(folder_path, csv_file_wide)

# 3. Definisi Kolom Target dan Fitur Input (SESUAIKAN JIKA PERLU)
target_column = 'Jakarta Pusat_Beras Kualitas Medium I'
feature_columns = [
    'Jakarta Pusat_Beras Kualitas Medium I', 'Jakarta Pusat_Beras Kualitas Medium I', 'Jakarta Pusat_Beras Kualitas Medium I', 'Jakarta Pusat_Beras Kualitas Medium I', 'Kota Bogor_Beras Kualitas Medium I',
    'Jakarta Pusat_Beras Kualitas Medium I', 'Jakarta Pusat_Beras Kualitas Medium I', 'Jakarta Pusat_Beras Kualitas Medium I', 'Jakarta Pusat_Beras Kualitas Medium I', 'Kota Bekasi_Beras Kualitas Medium I'
]

# 4. Proporsi Pembagian Data
train_split_prop = 0.70 # 70% untuk training
val_split_prop = 0.15   # 15% untuk validasi
# Sisa (1 - 0.70 - 0.15 = 0.15) otomatis menjadi test set (15%)

# --- Memuat Data Wide ---
df_wide = None
print("Mencoba memuat data wide yang sudah diimputasi...")
# (Kode pemuatan data sama seperti sebelumnya - prioritaskan Parquet)
if os.path.exists(wide_parquet_path):
    try:
        print(f"Memuat dari Parquet: {wide_parquet_path}")
        df_wide = pd.read_parquet(wide_parquet_path)
        if not isinstance(df_wide.index, pd.DatetimeIndex):
             df_wide.index = pd.to_datetime(df_wide.index)
        print("Berhasil memuat dari Parquet.")
    except Exception as e:
        print(f"Gagal memuat Parquet: {e}. Mencoba CSV...")
        df_wide = None

if df_wide is None and os.path.exists(wide_csv_path):
    try:
        print(f"Memuat dari CSV: {wide_csv_path}")
        df_wide = pd.read_csv(wide_csv_path, index_col='Tanggal', parse_dates=True)
        print("Berhasil memuat dari CSV.")
    except Exception as e:
        print(f"Gagal memuat CSV: {e}")
        df_wide = None

if df_wide is None:
    print("\nError: Tidak dapat memuat data wide gabungan.")
else:
    print("\nData wide berhasil dimuat.")

    # --- Langkah 4.1: Pemilihan Fitur Input (X) dan Target (y) ---
    print("\nLangkah 4.1: Memilih Fitur Input (X) dan Target (y)...")

    # Pastikan semua kolom yang dibutuhkan ada
    all_needed_columns = list(set([target_column] + feature_columns))
    missing_cols = [col for col in all_needed_columns if col not in df_wide.columns]

    if missing_cols:
        print(f"\nError: Kolom berikut tidak ditemukan di DataFrame: {missing_cols}")
        print("Pastikan nama kolom di 'target_column' dan 'feature_columns' sudah benar.")
    else:
        print(f"  Target (y): '{target_column}'")
        print(f"  Fitur Input (X): {feature_columns}")

        # Membuat DataFrame X dan Series y
        X = df_wide[feature_columns].copy()
        y = df_wide[target_column].copy()

        # --- Langkah 4.2: Pembagian Data (Train/Validation/Test Split) ---
        print("\nLangkah 4.2: Melakukan Pembagian Data secara Kronologis...")

        n_total = len(X)
        n_train = int(n_total * train_split_prop)
        n_val = int(n_total * val_split_prop)
        # n_test = n_total - n_train - n_val # Sisa data

        # Melakukan slicing berdasarkan posisi integer (iloc) karena index sudah urut waktu
        X_train = X.iloc[:n_train]
        y_train = y.iloc[:n_train]

        X_val = X.iloc[n_train : n_train + n_val]
        y_val = y.iloc[n_train : n_train + n_val]

        X_test = X.iloc[n_train + n_val:]
        y_test = y.iloc[n_train + n_val:]

        # --- Verifikasi Hasil Split ---
        print("\nVerifikasi Hasil Pembagian Data:")
        print(f"  Total data     : {n_total} baris")
        print(f"  Training Set   : {len(X_train)} baris ({len(X_train)/n_total:.1%}) | Tanggal: {X_train.index.min().strftime('%Y-%m-%d')} s/d {X_train.index.max().strftime('%Y-%m-%d')}")
        print(f"  Validation Set : {len(X_val)} baris ({len(X_val)/n_total:.1%}) | Tanggal: {X_val.index.min().strftime('%Y-%m-%d')} s/d {X_val.index.max().strftime('%Y-%m-%d')}")
        print(f"  Test Set       : {len(X_test)} baris ({len(X_test)/n_total:.1%}) | Tanggal: {X_test.index.min().strftime('%Y-%m-%d')} s/d {X_test.index.max().strftime('%Y-%m-%d')}")

        # Tampilkan shape untuk memastikan dimensi benar
        print("\nDimensi (Shape) Data Hasil Split:")
        print(f"  X_train: {X_train.shape}, y_train: {y_train.shape}")
        print(f"  X_val:   {X_val.shape}, y_val:   {y_val.shape}")
        print(f"  X_test:  {X_test.shape}, y_test:  {y_test.shape}")

        print("\nData siap untuk tahap Scaling dan Sequencing.")

# Anda sekarang memiliki: X_train, y_train, X_val, y_val, X_test, y_test
# Pastikan untuk menyimpan variabel-variabel ini jika Anda menjalankan di notebook
# atau muat kembali data wide dan jalankan kode ini lagi sebelum lanjut ke scaling.

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler # Import scaler
import os

# --- Pastikan Variabel Hasil Split Tersedia ---
# Jika Anda menjalankan ini di sel/skrip terpisah, muat ulang data wide
# dan jalankan lagi kode splitting dari langkah sebelumnya.
# Kode ini berasumsi X_train, y_train, dkk. sudah ada.
if 'X_train' not in locals() or 'y_train' not in locals():
     print("Error: Variabel X_train/y_train dkk. tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Data Splitting sebelumnya.")
     # Berhenti jika variabel tidak ada
     # exit() # Uncomment jika menjalankan sebagai skrip mandiri

else:
    print("Memulai Langkah 4.3: Scaling Data...")

    # --- Membuat Scaler ---
    # 1. Scaler untuk Fitur Input (X)
    x_scaler = MinMaxScaler(feature_range=(0, 1)) # Skala ke rentang 0-1

    # 2. Scaler untuk Target (y) - Scaler terpisah penting untuk inverse transform
    y_scaler = MinMaxScaler(feature_range=(0, 1))

    # --- Scaling Data ---
    # PENTING: Fit HANYA pada data training!

    # 1. Scale X_train (Fit dan Transform)
    print("  Scaling X_train...")
    # fit_transform mengembalikan NumPy array
    X_train_scaled = x_scaler.fit_transform(X_train)
    print(f"    Shape X_train_scaled: {X_train_scaled.shape}")

    # 2. Scale y_train (Fit dan Transform)
    print("  Scaling y_train...")
    # y_train adalah Series, perlu di-reshape jadi 2D untuk scaler
    # .values mengubah ke NumPy array, .reshape(-1, 1) menjadikannya kolom tunggal
    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))
    print(f"    Shape y_train_scaled: {y_train_scaled.shape}")

    # 3. Scale X_val (Hanya Transform, menggunakan x_scaler yang sudah di-fit)
    print("  Scaling X_val...")
    X_val_scaled = x_scaler.transform(X_val)
    print(f"    Shape X_val_scaled: {X_val_scaled.shape}")

    # 4. Scale y_val (Hanya Transform, menggunakan y_scaler yang sudah di-fit)
    print("  Scaling y_val...")
    y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1))
    print(f"    Shape y_val_scaled: {y_val_scaled.shape}")

    # 5. Scale X_test (Hanya Transform, menggunakan x_scaler yang sudah di-fit)
    print("  Scaling X_test...")
    X_test_scaled = x_scaler.transform(X_test)
    print(f"    Shape X_test_scaled: {X_test_scaled.shape}")

    # 6. Scale y_test (Hanya Transform, menggunakan y_scaler yang sudah di-fit)
    print("  Scaling y_test...")
    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))
    print(f"    Shape y_test_scaled: {y_test_scaled.shape}")

    # --- Verifikasi Scaling (Opsional) ---
    print("\nVerifikasi Scaling (Contoh X_train_scaled):")
    print(f"  Min value: {np.min(X_train_scaled):.4f}")
    print(f"  Max value: {np.max(X_train_scaled):.4f}")
    # Cetak min/max per kolom jika ingin lebih detail
    print(f"  Min per kolom: {np.min(X_train_scaled, axis=0)}")
    print(f"  Max per kolom: {np.max(X_train_scaled, axis=0)}")

    print("\nScaling data selesai.")
    print("Variabel hasil: X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_test_scaled")
    print("PENTING: Simpan objek 'x_scaler' dan 'y_scaler' jika Anda perlu memuat model nanti!")
    # Contoh menyimpan scaler (jika diperlukan):
    import joblib
    joblib.dump(x_scaler, os.path.join(folder_path, 'x_scaler.gz'))
    joblib.dump(y_scaler, os.path.join(folder_path, 'y_scaler.gz'))

import numpy as np
import pandas as pd # Hanya untuk verifikasi jika perlu

# --- Konfigurasi Windowing ---
# Tentukan jumlah langkah waktu sebelumnya (hari) yang akan digunakan
# untuk memprediksi hari berikutnya.
# Nilai umum bisa 30, 60, 90, dll. Perlu eksperimen.
look_back = 60 # Contoh: menggunakan 60 hari terakhir untuk prediksi hari ke-61

# --- Pastikan Variabel Hasil Scaling Tersedia ---
# Asumsi X_train_scaled, y_train_scaled, dkk. sudah ada dari langkah sebelumnya
if 'X_train_scaled' not in locals() or 'y_train_scaled' not in locals():
     print("Error: Variabel ..._scaled tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Scaling sebelumnya.")
     # exit() # Uncomment jika perlu

else:
    print(f"Memulai Langkah 4.4: Pembuatan Sequence (Windowing) dengan look_back={look_back}...")

    # --- Fungsi untuk Membuat Sequence ---
    def create_sequences(data_x, data_y, time_steps=1):
        """
        Mengubah data time series menjadi format sequence untuk LSTM.
        Args:
            data_x (np.array): Array fitur input (scaled). Shape: [n_samples, n_features]
            data_y (np.array): Array target (scaled). Shape: [n_samples, 1]
            time_steps (int): Jumlah langkah waktu sebelumnya (look_back).
        Returns:
            np.array: Array X dalam format sequence [n_sequences, time_steps, n_features]
            np.array: Array y dalam format sequence [n_sequences, 1]
        """
        output_x = []
        output_y = []
        # Iterasi data, berhenti 'time_steps' sebelum akhir agar target y selalu ada
        for i in range(len(data_x) - time_steps):
            # Ambil sequence input X sepanjang 'time_steps'
            sequence_x = data_x[i:(i + time_steps)]
            output_x.append(sequence_x)

            # Ambil target y pada langkah waktu setelah sequence X
            target_y = data_y[i + time_steps]
            output_y.append(target_y)

        return np.array(output_x), np.array(output_y)

    # --- Membuat Sequence untuk Setiap Set Data ---

    print("  Membuat sequence untuk Training Set...")
    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, look_back)
    print(f"    Shape X_train_seq: {X_train_seq.shape}")
    print(f"    Shape y_train_seq: {y_train_seq.shape}")

    print("  Membuat sequence untuk Validation Set...")
    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, look_back)
    print(f"    Shape X_val_seq: {X_val_seq.shape}")
    print(f"    Shape y_val_seq: {y_val_seq.shape}")

    print("  Membuat sequence untuk Test Set...")
    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, look_back)
    print(f"    Shape X_test_seq: {X_test_seq.shape}")
    print(f"    Shape y_test_seq: {y_test_seq.shape}")

    # --- Verifikasi (Opsional) ---
    # Cek apakah jumlah sequence masuk akal (original_length - look_back)
    expected_train_seq_len = len(X_train_scaled) - look_back
    if X_train_seq.shape[0] == expected_train_seq_len:
        print("\n  Verifikasi panjang sequence training: OK")
    else:
        print(f"\n  Peringatan: Panjang sequence training ({X_train_seq.shape[0]}) tidak sesuai harapan ({expected_train_seq_len}).")

    print("\nPembuatan sequence selesai.")
    print("Variabel hasil: X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq")
    print("Data siap untuk dimasukkan ke model LSTM.")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
import os

# --- Konfigurasi Model ---
# Kita perlu tahu bentuk input dari data sequence yang sudah dibuat
# Asumsi variabel ini sudah ada dari langkah windowing:
# X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq

# Ambil dimensi dari X_train_seq (jika sudah dibuat)
# Jika belum, Anda perlu menjalankannya atau mendefinisikan look_back dan n_features secara manual
try:
    # Shape: [jumlah_sampel, timesteps/look_back, jumlah_fitur]
    input_timesteps = X_train_seq.shape[1]
    input_features = X_train_seq.shape[2]
    print(f"Input shape terdeteksi: timesteps={input_timesteps}, features={input_features}")
except NameError:
    print("Error: Variabel X_train_seq tidak ditemukan. Menggunakan nilai default (harap sesuaikan!).")
    # --> GANTI INI JIKA PERLU <--
    look_back_default = 60 # Harus sama dengan look_back saat membuat sequence
    n_features_default = 5 # Harus sama dengan jumlah kolom di feature_columns
    input_timesteps = look_back_default
    input_features = n_features_default
    print(f"Menggunakan nilai default: timesteps={input_timesteps}, features={input_features}")


# Hyperparameter Model Awal (bisa di-tuning nanti)
lstm_units_1 = 50   # Jumlah neuron di layer LSTM pertama
dropout_rate = 0.2  # Rate untuk dropout layer (mencegah overfitting)
# lstm_units_2 = 50 # Uncomment jika ingin menambah layer LSTM kedua
optimizer = 'adam'      # Algoritma optimasi
loss_function = 'mean_squared_error' # Fungsi loss untuk regresi
metrics_to_monitor = ['mean_absolute_error'] # Metrik tambahan untuk dipantau

# --- Membangun Arsitektur Model ---
print("\nMembangun arsitektur model LSTM...")

model = Sequential(name="LSTM_Multivariate_HargaPangan")

# Input Layer (implisit dari layer pertama) + Layer LSTM Pertama
# input_shape = (jumlah langkah waktu, jumlah fitur per langkah waktu)
model.add(Input(shape=(input_timesteps, input_features), name='Input_Layer')) # Cara eksplisit mendefinisikan input
model.add(LSTM(units=lstm_units_1,
               # return_sequences=True, # Set True jika ada layer LSTM lain setelah ini
               name='LSTM_Layer_1'))

# Layer Dropout
model.add(Dropout(dropout_rate, name='Dropout_Layer_1'))

# Layer Output Dense
# units=1 karena kita memprediksi satu nilai (harga target)
# activation='linear' (default) cocok untuk regresi
model.add(Dense(units=1, name='Output_Layer'))

# --- Kompilasi Model ---
print(f"Mengompilasi model dengan optimizer='{optimizer}' dan loss='{loss_function}'...")
model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics_to_monitor)

# --- Tampilkan Ringkasan Model ---
print("\nRingkasan Arsitektur Model:")
model.summary()

print("\nModel LSTM berhasil dibangun dan dikompilasi.")
print("Model siap untuk dilatih menggunakan model.fit()")

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt
from plotly.subplots import make_subplots
import os
import time # Untuk mengukur waktu training

# --- Konfigurasi Training ---
# Hyperparameter training (bisa di-tuning)
epochs = 50        # Jumlah maksimum iterasi training (EarlyStopping bisa menghentikan lebih awal)
batch_size = 32    # Jumlah sampel yang diproses sebelum bobot diperbarui

# Konfigurasi Callbacks
# 1. Early Stopping: Menghentikan training jika loss validasi tidak membaik
early_stopping_patience = 10 # Berapa epoch menunggu perbaikan sebelum berhenti
early_stopping_monitor = 'val_loss' # Pantau loss di set validasi

# 2. Model Checkpoint: Menyimpan model terbaik (berdasarkan val_loss)
#    Tentukan path folder dan nama file untuk menyimpan model terbaik
model_checkpoint_filepath = os.path.join(folder_path, 'lstm_multivariate_best_model.keras') # Gunakan .keras
model_checkpoint_monitor = 'val_loss'

# 3. Reduce Learning Rate on Plateau (Opsional tapi sering membantu)
#    Mengurangi learning rate jika loss validasi stagnan
reduce_lr_factor = 0.2   # Faktor pengurangan LR (new_lr = lr * factor)
reduce_lr_patience = 5   # Berapa epoch menunggu sebelum mengurangi LR
reduce_lr_monitor = 'val_loss'

# --- Pastikan Variabel Sequence Tersedia ---
# Asumsi X_train_seq, y_train_seq, X_val_seq, y_val_seq sudah ada
if 'X_train_seq' not in locals() or 'y_train_seq' not in locals():
    print("Error: Variabel ..._seq tidak ditemukan.")
    print("Pastikan Anda sudah menjalankan kode Windowing sebelumnya.")
    # exit()
elif 'model' not in locals():
    print("Error: Variabel 'model' tidak ditemukan.")
    print("Pastikan Anda sudah menjalankan kode Membangun & Kompilasi Model sebelumnya.")
    # exit()
else:
    print("Memulai Langkah Training Model...")

    # --- Menyiapkan Callbacks ---
    early_stopping = EarlyStopping(monitor=early_stopping_monitor,
                                 patience=early_stopping_patience,
                                 verbose=1, # Cetak pesan saat berhenti
                                 restore_best_weights=True) # Kembalikan bobot terbaik saat berhenti

    model_checkpoint = ModelCheckpoint(filepath=model_checkpoint_filepath,
                                       monitor=model_checkpoint_monitor,
                                       save_best_only=True, # Hanya simpan yang terbaik
                                       verbose=1) # Cetak pesan saat menyimpan model

    reduce_lr = ReduceLROnPlateau(monitor=reduce_lr_monitor,
                                factor=reduce_lr_factor,
                                patience=reduce_lr_patience,
                                verbose=1, # Cetak pesan saat LR dikurangi
                                min_lr=1e-6) # Learning rate minimum

    callbacks_list = [early_stopping, model_checkpoint, reduce_lr]

    # --- Melatih Model ---
    print("\nMelatih model...")
    start_time = time.time()

    history = model.fit(X_train_seq, y_train_seq,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(X_val_seq, y_val_seq),
                        callbacks=callbacks_list,
                        verbose=1) # verbose=1 untuk progress bar, verbose=2 untuk satu baris per epoch

    end_time = time.time()
    training_time = end_time - start_time
    print(f"\nTraining selesai dalam {training_time:.2f} detik.")

    # --- Visualisasi Hasil Training ---
    print("\nMembuat plot histori training (Loss & Metrik)...")

    history_df = pd.DataFrame(history.history)

    fig = make_subplots(rows=1, cols=2, subplot_titles=('Model Loss (MSE)', f'Model Metric ({metrics_to_monitor[0]})'))

    # Plot Loss
    fig.add_trace(go.Scatter(y=history_df['loss'], name='Training Loss', mode='lines'), row=1, col=1)
    fig.add_trace(go.Scatter(y=history_df['val_loss'], name='Validation Loss', mode='lines'), row=1, col=1)

    # Plot Metrik (misal, MAE)
    metric_name = metrics_to_monitor[0] # Ambil nama metrik pertama yang dimonitor
    if metric_name in history_df.columns:
         fig.add_trace(go.Scatter(y=history_df[metric_name], name=f'Training {metric_name}', mode='lines'), row=1, col=2)
         fig.add_trace(go.Scatter(y=history_df[f'val_{metric_name}'], name=f'Validation {metric_name}', mode='lines'), row=1, col=2)
    else:
         print(f"Peringatan: Metrik '{metric_name}' tidak ditemukan dalam histori.")


    fig.update_layout(title_text='Histori Training Model LSTM', height=400)
    fig.update_xaxes(title_text='Epoch', row=1, col=1)
    fig.update_xaxes(title_text='Epoch', row=1, col=2)
    fig.update_yaxes(title_text='Loss (MSE)', row=1, col=1)
    fig.update_yaxes(title_text=f'Metric ({metrics_to_monitor[0]})', row=1, col=2)
    fig.show()

    print("\nModel terbaik disimpan di:", model_checkpoint_filepath)
    print("Langkah selanjutnya: Evaluasi model pada Test Set.")

    # Simpan history untuk referensi (opsional)
    # history_df.to_csv(os.path.join(folder_path, 'training_history.csv'))

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import plotly.graph_objects as go
import os

# --- Pastikan Variabel/Objek Tersedia ---
# Asumsi variabel/objek ini sudah ada dari langkah-langkah sebelumnya:
# - X_test_seq: Data fitur test (scaled, sequenced)
# - y_test_seq: Data target test (scaled, sequenced) - BENTUK ASLI y_test_seq PENTING
# - y_scaler: Objek MinMaxScaler yang sudah di-fit pada y_train
# - model_checkpoint_filepath: Path ke file .keras model terbaik
# - y_test: Series PANDAS target test ASLI (unscaled, unsequenced) - untuk plotting & perbandingan
# - look_back: Jumlah timestep windowing
# - target_column: Nama kolom target

if 'X_test_seq' not in locals() or 'y_test_seq' not in locals():
    print("Error: Variabel X_test_seq/y_test_seq tidak ditemukan.")
    print("Pastikan Anda sudah menjalankan kode Windowing sebelumnya.")
    # exit()
elif 'y_scaler' not in locals():
     print("Error: Variabel 'y_scaler' tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Scaling sebelumnya.")
     # exit()
elif 'model_checkpoint_filepath' not in locals() or not os.path.exists(model_checkpoint_filepath):
     print(f"Error: File model terbaik tidak ditemukan di {model_checkpoint_filepath}")
     print("Pastikan path sudah benar dan model sudah disimpan saat training.")
     # exit()
elif 'y_test' not in locals():
     print(f"Error: Variabel y_test (data asli unscaled) tidak ditemukan.")
     print("Pastikan Anda sudah menjalankan kode Data Splitting.")
     # exit()

else:
    print("Memulai Evaluasi Model pada Test Set...")

    # --- 1. Muat Model Terbaik ---
    print(f"Memuat model terbaik dari: {model_checkpoint_filepath}")
    try:
        best_model = tf.keras.models.load_model(model_checkpoint_filepath)
        print("Model berhasil dimuat.")
        best_model.summary() # Tampilkan summary untuk konfirmasi
    except Exception as e:
        print(f"Error saat memuat model: {e}")
        # exit()
        best_model = None # Set ke None jika gagal

    if best_model:
        # --- 2. Lakukan Prediksi pada Test Set ---
        print("\nMelakukan prediksi pada X_test_seq...")
        y_pred_scaled = best_model.predict(X_test_seq)
        print(f"Shape hasil prediksi (scaled): {y_pred_scaled.shape}")

        # --- 3. Inverse Transform Prediksi ---
        print("Melakukan inverse transform pada hasil prediksi...")
        # y_pred_scaled berbentuk [n_samples, 1], cocok untuk scaler
        y_pred_actual = y_scaler.inverse_transform(y_pred_scaled)
        print(f"Shape hasil prediksi (actual scale): {y_pred_actual.shape}")
        # Ubah ke 1D array untuk perhitungan metrik yang lebih mudah
        y_pred_actual_1d = y_pred_actual.flatten()

        # --- 4. Inverse Transform Nilai y_test_seq (Target Sebenarnya) ---
        print("Melakukan inverse transform pada nilai y_test_seq (target sebenarnya)...")
        # y_test_seq mungkin berbentuk [n_samples, 1] jika hasil dari create_sequences
        # Jika tidak, reshape dulu
        if y_test_seq.ndim == 1:
             y_test_seq_reshaped = y_test_seq.reshape(-1, 1)
        else:
             y_test_seq_reshaped = y_test_seq

        y_test_actual = y_scaler.inverse_transform(y_test_seq_reshaped)
        print(f"Shape target test (actual scale): {y_test_actual.shape}")
        # Ubah ke 1D array
        y_test_actual_1d = y_test_actual.flatten()

        # --- 5. Hitung Metrik Evaluasi ---
        print("\nMenghitung Metrik Evaluasi (pada skala asli):")
        mae = mean_absolute_error(y_test_actual_1d, y_pred_actual_1d)
        rmse = np.sqrt(mean_squared_error(y_test_actual_1d, y_pred_actual_1d))
        mape = mean_absolute_percentage_error(y_test_actual_1d, y_pred_actual_1d) * 100 # Kali 100 untuk persentase

        print(f"  Mean Absolute Error (MAE): {mae:,.2f} (Rata-rata error absolut dalam Rupiah)")
        print(f"  Root Mean Squared Error (RMSE): {rmse:,.2f} (Akar rata-rata kuadrat error dalam Rupiah)")
        print(f"  Mean Absolute Percentage Error (MAPE): {mape:.2f}% (Rata-rata persentase error)")

        # --- 6. Visualisasi Prediksi vs Aktual ---
        print("\nMembuat plot perbandingan Prediksi vs Aktual pada Test Set...")

        # Kita butuh tanggal yang sesuai dengan y_test_actual_1d dan y_pred_actual_1d
        # Panjang y_test_actual_1d adalah len(y_test) - look_back
        # Jadi, kita ambil tanggal dari y_test ASLI (unscaled) mulai dari index ke 'look_back'
        if len(y_test_actual_1d) == len(y_test) - look_back:
            dates_for_plot = y_test.index[look_back:]

            # Membuat DataFrame untuk plotting
            plot_df = pd.DataFrame({
                'Tanggal': dates_for_plot,
                'Harga Aktual': y_test_actual_1d,
                'Harga Prediksi': y_pred_actual_1d
            })

            fig_eval = go.Figure()
            fig_eval.add_trace(go.Scatter(x=plot_df['Tanggal'], y=plot_df['Harga Aktual'],
                                        mode='lines', name='Harga Aktual', line=dict(color='blue')))
            fig_eval.add_trace(go.Scatter(x=plot_df['Tanggal'], y=plot_df['Harga Prediksi'],
                                        mode='lines', name='Harga Prediksi LSTM', line=dict(color='red', dash='dash')))

            fig_eval.update_layout(
                title=f'Perbandingan Harga Aktual vs Prediksi LSTM - Test Set<br>({target_column})',
                xaxis_title='Tanggal',
                yaxis_title='Harga (Rp)',
                hovermode='x unified'
            )
            fig_eval.show()
            print("Plot evaluasi ditampilkan.")

        else:
             print("Peringatan: Panjang data prediksi dan target test tidak cocok dengan y_test asli setelah look_back. Tidak dapat membuat plot tanggal yang benar.")


    print("\n--- Evaluasi Model Selesai ---")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
from tensorflow.keras.callbacks import EarlyStopping # ModelCheckpoint tidak praktis untuk ratusan model
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import os
import time
import joblib # Untuk menyimpan scaler JIKA Anda memutuskan menyimpan model individu nanti

# --- Helper Function: Create Sequences (Sama seperti sebelumnya, tapi y adalah bagian dari X) ---
def create_univariate_sequences(data_scaled, time_steps=1):
    """
    Mengubah data time series univariate menjadi format sequence untuk LSTM.
    Args:
        data_scaled (np.array): Array target/fitur input (scaled). Shape: [n_samples, 1]
        time_steps (int): Jumlah langkah waktu sebelumnya (look_back).
    Returns:
        np.array: Array X dalam format sequence [n_sequences, time_steps, 1]
        np.array: Array y dalam format sequence [n_sequences, 1]
    """
    output_x, output_y = [], []
    # Iterasi data, berhenti 'time_steps' sebelum akhir
    for i in range(len(data_scaled) - time_steps):
        # Ambil sequence input X sepanjang 'time_steps'
        sequence_x = data_scaled[i:(i + time_steps)]
        output_x.append(sequence_x)
        # Ambil target y pada langkah waktu setelah sequence X
        target_y = data_scaled[i + time_steps]
        output_y.append(target_y)
    return np.array(output_x), np.array(output_y)

# --- Fungsi Eksperimen Univariate Utama ---
def run_univariate_lstm_experiments(
    df_wide_imputed,               # DataFrame wide yang sudah bersih & imputasi
    # -- Parameter Split --
    train_split_prop=0.7,
    val_split_prop=0.15,
    # -- Parameter LSTM Model --
    look_back=60,
    lstm_units=50,
    dropout_rate=0.2,
    optimizer='adam',
    loss_function='mean_squared_error',
    # -- Parameter Training --
    epochs=50,
    batch_size=32,
    early_stopping_patience=10,
    # -- Path Penyimpanan Hasil --
    base_folder_path='/content/drive/MyDrive/project02',
    results_filename="lstm_univariate_experiment_results.csv"
    ):
    """
    Menjalankan eksperimen LSTM UNIVARIATE untuk SETIAP kolom di DataFrame.
    """
    all_results = []

    print(f"Memulai Eksperimen LSTM Univariate untuk {len(df_wide_imputed.columns)} kolom...")
    print("==================================================================")

    # Loop melalui setiap kolom (setiap deret waktu)
    for target_column in df_wide_imputed.columns:
        print(f"\n--- Memproses Target: {target_column} ---")
        start_time_exp = time.time()

        # 1. Ambil Data untuk target saat ini
        ts_data = df_wide_imputed[target_column].copy()

        # 2. Split Data (Kronologis)
        n_total = len(ts_data)
        n_train = int(n_total * train_split_prop)
        n_val = int(n_total * val_split_prop)

        train_data = ts_data.iloc[:n_train]
        val_data = ts_data.iloc[n_train : n_train + n_val]
        test_data = ts_data.iloc[n_train + n_val:]
        print(f"   Data split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")

        # 3. Scaling Data
        scaler = MinMaxScaler(feature_range=(0, 1)) # Cukup satu scaler per time series
        # Reshape ke 2D untuk scaler
        train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))
        val_scaled = scaler.transform(val_data.values.reshape(-1, 1))
        test_scaled = scaler.transform(test_data.values.reshape(-1, 1))
        print("   Scaling OK.")

        # 4. Membuat Sequence (Gunakan fungsi univariate)
        print("   Membuat Sequence...")
        X_train_seq, y_train_seq = create_univariate_sequences(train_scaled, look_back)
        X_val_seq, y_val_seq = create_univariate_sequences(val_scaled, look_back)
        X_test_seq, y_test_seq_scaled = create_univariate_sequences(test_scaled, look_back)

        if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:
            print("   Error: Tidak cukup data untuk membuat sequence. Melewati.")
            all_results.append({'Target': target_column, 'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan,
                                'Best_Epoch': np.nan, 'Duration_s': time.time() - start_time_exp,
                                'Status': 'Error: Insufficient Data for Sequencing'})
            continue
        print("   Sequencing OK.")

        # 5. Bangun & Kompilasi Model
        tf.keras.backend.clear_session() # Bersihkan sesi Keras
        print("   Membangun & Kompilasi Model...")
        # Original: model = Sequential(name=f"LSTM_Univar_{target_column[:20]}") # Nama pendek
        # --> PERBAIKAN: Ganti spasi di nama target untuk nama model <--
        model_name_safe = f"LSTM_Univar_{target_column.replace(' ', '_')[:30]}" # Ganti spasi & batasi panjang
        print(f"     Nama model: {model_name_safe}")
        model = Sequential(name=model_name_safe)
        # Input shape sekarang (look_back, 1) karena hanya 1 fitur
        model.add(Input(shape=(look_back, 1), name='Input_Layer'))
        model.add(LSTM(units=lstm_units, name='LSTM_Layer_1'))
        model.add(Dropout(dropout_rate, name='Dropout_Layer_1'))
        model.add(Dense(units=1, name='Output_Layer'))
        model.compile(optimizer=optimizer, loss=loss_function, metrics=['mean_absolute_error'])

        # 6. Siapkan Callbacks (Hanya EarlyStopping, checkpoint tidak praktis)
        es = EarlyStopping(monitor='val_loss', patience=early_stopping_patience,
                         restore_best_weights=True, verbose=0)

        # 7. Latih Model
        print("   Melatih Model...")
        history = model.fit(X_train_seq, y_train_seq,
                            epochs=epochs,
                            batch_size=batch_size,
                            validation_data=(X_val_seq, y_val_seq),
                            callbacks=[es], # Hanya EarlyStopping
                            verbose=0)
        best_epoch = np.argmin(history.history['val_loss']) + 1
        print(f"   Training selesai. Best epoch (val_loss): {best_epoch}")

        # 8. Evaluasi Model (menggunakan bobot terbaik yg direstore oleh EarlyStopping)
        print("   Mengevaluasi model pada Test Set...")
        y_pred_scaled = model.predict(X_test_seq)

        # Inverse Transform
        try:
            y_pred_actual = scaler.inverse_transform(y_pred_scaled)
            y_test_actual = scaler.inverse_transform(y_test_seq_scaled) # Inverse target test sequence
        except Exception as scale_err:
             print(f"    Error saat inverse transform: {scale_err}. Melewati evaluasi.")
             all_results.append({'Target': target_column, 'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan,
                                'Best_Epoch': best_epoch, 'Duration_s': time.time() - start_time_exp,
                                'Status': 'Error: Inverse Transform Failed'})
             continue

        # Align lengths
        min_len = min(len(y_pred_actual), len(y_test_actual))
        y_pred_actual_aligned = y_pred_actual[:min_len].flatten()
        y_test_actual_aligned = y_test_actual[:min_len].flatten()

        if len(y_test_actual_aligned) == 0:
             print("    Error: Tidak ada data test yang cocok setelah alignment. Melewati evaluasi.")
             all_results.append({'Target': target_column, 'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan,
                                'Best_Epoch': best_epoch, 'Duration_s': time.time() - start_time_exp,
                                'Status': 'Error: Alignment resulted in zero length'})
             continue

        # Hitung Metrik
        mae_test = mean_absolute_error(y_test_actual_aligned, y_pred_actual_aligned)
        rmse_test = np.sqrt(mean_squared_error(y_test_actual_aligned, y_pred_actual_aligned))
        mape_test = mean_absolute_percentage_error(y_test_actual_aligned, y_pred_actual_aligned) * 100

        end_time_exp = time.time()
        duration_exp = end_time_exp - start_time_exp
        print(f"   MAE Test : {mae_test:.2f}")
        print(f"   RMSE Test: {rmse_test:.2f}")
        print(f"   MAPE Test: {mape_test:.2f}%")
        print(f"   Durasi   : {duration_exp:.1f} detik")

        # 9. Simpan Hasil
        all_results.append({
            'Target': target_column,
            'MAE': mae_test,
            'RMSE': rmse_test,
            'MAPE': mape_test,
            'Best_Epoch': best_epoch,
            'Duration_s': duration_exp,
            'Status': 'Completed'
        })

    # --- Selesai Loop Kolom ---
    print("\n==================================================================")
    print("Semua Eksperimen Univariate Selesai.")

    if not all_results:
        print("Tidak ada hasil eksperimen untuk ditampilkan atau disimpan.")
        return None

    results_df = pd.DataFrame(all_results)
    results_df.sort_values(by='MAPE', inplace=True) # Urutkan

    print("\nHasil Eksperimen (Diurutkan berdasarkan MAPE):")
    with pd.option_context('display.max_rows', 10, 'display.max_columns', None): # Tampilkan 10 teratas
        print(results_df)

    results_path = os.path.join(base_folder_path, results_filename)
    try:
        print(f"\nMenyimpan hasil ringkasan ke: {results_path}")
        results_df.to_csv(results_path, index=False)
        print("Hasil berhasil disimpan.")
    except Exception as e:
        print(f"Gagal menyimpan hasil: {e}")

    return results_df

# --- Cara Menggunakan Fungsi ---

# 1. Muat Data Wide yang Sudah Diimputasi
print("Memuat data wide imputed...")
wide_parquet_path = os.path.join(folder_path, 'data_pangan_jabodetabek_wide_imputed.parquet') # Pastikan nama file benar
if os.path.exists(wide_parquet_path):
    df_wide_imputed = pd.read_parquet(wide_parquet_path)
    df_wide_imputed.index = pd.to_datetime(df_wide_imputed.index)
    print(f"Data berhasil dimuat. Shape: {df_wide_imputed.shape}")
else:
    print(f"Error: File {wide_parquet_path} tidak ditemukan!")
    df_wide_imputed = None

# (OPSIONAL) Jalankan pada subset kolom untuk testing cepat
# columns_to_run = df_wide_imputed.columns[:10] # Ambil 10 kolom pertama
# df_subset = df_wide_imputed[columns_to_run]
# print(f"\nMenjalankan eksperimen hanya pada subset {len(columns_to_run)} kolom...")
# if df_subset is not None:
#      univariate_results = run_univariate_lstm_experiments(df_subset, look_back=30, epochs=10) # Epochs lebih sedikit untuk test


# 2. Jalankan Eksperimen untuk SEMUA kolom (Akan sangat lama!)
if df_wide_imputed is not None:
     print("\nMemulai Eksekusi Fungsi Eksperimen Univariate untuk SEMUA Kolom...")
     univariate_results = run_univariate_lstm_experiments(
         df_wide_imputed=df_wide_imputed,
         look_back=60, # Sesuaikan hyperparameter jika perlu
         lstm_units=50,
         epochs=50,
         early_stopping_patience=10
     )
     print("\nFungsi Eksperimen Univariate Selesai.")
     if univariate_results is not None:
         print("\nDataFrame Hasil Akhir (10 teratas berdasarkan MAPE):")
         print(univariate_results.head(10))
else:
     print("\nTidak dapat menjalankan eksperimen karena data tidak dimuat.")

"""# **==== LSTM UNIVARIATE (SAVE MODEL) ====**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
# Sekarang kita butuh ModelCheckpoint lagi
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import os
import time
import joblib # Untuk menyimpan/memuat scaler
import re # Untuk membersihkan nama file

folder_path = r'/content/drive/MyDrive/project02'

# --- Helper Function: Create Sequences (Univariate) ---
def create_univariate_sequences(data_scaled, time_steps=1):
    output_x, output_y = [], []
    for i in range(len(data_scaled) - time_steps):
        sequence_x = data_scaled[i:(i + time_steps)]
        target_y = data_scaled[i + time_steps]
        output_x.append(sequence_x)
        output_y.append(target_y)
    return np.array(output_x), np.array(output_y)

# --- Fungsi Eksperimen Utama (dengan Penyimpanan Model & Scaler) ---
def train_and_save_univariate_models(
    df_wide_imputed,
    # -- Parameter Split --
    train_split_prop=0.7,
    val_split_prop=0.15,
    # -- Parameter LSTM Model --
    look_back=60,
    lstm_units=50,
    dropout_rate=0.2,
    optimizer='adam',
    loss_function='mean_squared_error',
    # -- Parameter Training --
    epochs=50,
    batch_size=32,
    early_stopping_patience=10,
    reduce_lr_patience=5,
    reduce_lr_factor=0.2,
    # -- Path Penyimpanan --
    base_folder_path='/content/drive/MyDrive/project02',
    # Nama subfolder untuk menyimpan model & scaler
    models_save_subfolder='univariate_models_saved',
    results_filename="lstm_univariate_training_log.csv" # Log training & evaluasi
    ):
    """
    Melatih dan MENYIMPAN model LSTM UNIVARIATE serta scaler untuk SETIAP kolom.
    """
    all_results_log = []
    models_folder = os.path.join(base_folder_path, models_save_subfolder)

    # Buat folder penyimpanan jika belum ada
    os.makedirs(models_folder, exist_ok=True)
    print(f"Model dan scaler akan disimpan di: {models_folder}")

    print(f"\nMemulai Training & Penyimpanan Model Univariate untuk {len(df_wide_imputed.columns)} kolom...")
    print("==================================================================")

    # Loop melalui setiap kolom (target)
    for target_column in df_wide_imputed.columns:
        print(f"\n--- Memproses Target: {target_column} ---")
        start_time_exp = time.time()

        # --- Persiapan Nama File yang Aman ---
        # Ganti karakter tidak valid (spasi, /, \) dengan _
        safe_col_name = re.sub(r'[\\/ ]', '_', target_column)
        # Batasi panjang jika perlu
        safe_col_name = safe_col_name[:80] # Batasi panjang nama file
        model_filename = f"model_univar_{safe_col_name}.keras"
        scaler_filename = f"scaler_univar_{safe_col_name}.gz"
        model_filepath = os.path.join(models_folder, model_filename)
        scaler_filepath = os.path.join(models_folder, scaler_filename)

        # 1. Ambil Data
        ts_data = df_wide_imputed[target_column].copy()

        # 2. Split Data
        n_total = len(ts_data)
        n_train = int(n_total * train_split_prop)
        n_val = int(n_total * val_split_prop)
        train_data = ts_data.iloc[:n_train]
        val_data = ts_data.iloc[n_train : n_train + n_val]
        test_data = ts_data.iloc[n_train + n_val:]
        print(f"   Data split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")

        # 3. Scaling Data
        scaler = MinMaxScaler(feature_range=(0, 1))
        train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))
        val_scaled = scaler.transform(val_data.values.reshape(-1, 1))
        test_scaled = scaler.transform(test_data.values.reshape(-1, 1))
        print("   Scaling OK.")

        # Simpan scaler SEKARANG
        try:
            joblib.dump(scaler, scaler_filepath)
            print(f"   Scaler disimpan ke: {scaler_filename}")
        except Exception as e:
            print(f"   Peringatan: Gagal menyimpan scaler: {e}")


        # 4. Membuat Sequence
        print("   Membuat Sequence...")
        X_train_seq, y_train_seq = create_univariate_sequences(train_scaled, look_back)
        X_val_seq, y_val_seq = create_univariate_sequences(val_scaled, look_back)
        X_test_seq, y_test_seq_scaled = create_univariate_sequences(test_scaled, look_back)

        if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:
            print("   Error: Tidak cukup data untuk sequencing. Melewati.")
            all_results_log.append({'Target': target_column, 'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan,
                                    'Best_Epoch': np.nan, 'Duration_s': time.time() - start_time_exp,
                                    'Status': 'Error: Insufficient Data', 'Model_File': None, 'Scaler_File': scaler_filename if 'scaler_filepath' in locals() else None})
            continue
        print("   Sequencing OK.")

        # 5. Bangun & Kompilasi Model
        tf.keras.backend.clear_session()
        model_name_safe = f"LSTM_Univar_{safe_col_name}" # Nama model aman
        model = Sequential(name=model_name_safe)
        model.add(Input(shape=(look_back, 1)))
        model.add(LSTM(units=lstm_units))
        model.add(Dropout(dropout_rate))
        model.add(Dense(units=1))
        model.compile(optimizer=optimizer, loss=loss_function, metrics=['mean_absolute_error'])
        print(f"   Model '{model_name_safe}' dibangun & dikompilasi.")

        # 6. Siapkan Callbacks (Termasuk ModelCheckpoint sekarang)
        es = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True, verbose=0)
        mc = ModelCheckpoint(filepath=model_filepath, # Simpan ke path yang sudah dibuat
                           monitor='val_loss', save_best_only=True, verbose=0) # Simpan yang terbaik
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_factor, patience=reduce_lr_patience, verbose=0, min_lr=1e-6)
        callbacks_list = [es, mc, reduce_lr]

        # 7. Latih Model
        print("   Melatih Model...")
        history = model.fit(X_train_seq, y_train_seq,
                            epochs=epochs,
                            batch_size=batch_size,
                            validation_data=(X_val_seq, y_val_seq),
                            callbacks=callbacks_list,
                            verbose=0)
        best_epoch = np.argmin(history.history['val_loss']) + 1
        print(f"   Training selesai. Best epoch (val_loss): {best_epoch}. Model terbaik disimpan di {model_filename}")

        # 8. Evaluasi Model Terbaik (Muat ulang dari file yg disimpan)
        print("   Mengevaluasi model tersimpan pada Test Set...")
        eval_status = 'Completed'
        mae_test, rmse_test, mape_test = np.nan, np.nan, np.nan # Default jika evaluasi gagal
        try:
            # Muat model yang disimpan oleh ModelCheckpoint
            best_model_loaded = tf.keras.models.load_model(model_filepath)
            y_pred_scaled = best_model_loaded.predict(X_test_seq)

            # Inverse Transform (Gunakan scaler yang sudah disimpan)
            loaded_scaler = joblib.load(scaler_filepath)
            y_pred_actual = loaded_scaler.inverse_transform(y_pred_scaled)
            y_test_actual = loaded_scaler.inverse_transform(y_test_seq_scaled)

            min_len = min(len(y_pred_actual), len(y_test_actual))
            y_pred_actual_aligned = y_pred_actual[:min_len].flatten()
            y_test_actual_aligned = y_test_actual[:min_len].flatten()

            if len(y_test_actual_aligned) > 0:
                mae_test = mean_absolute_error(y_test_actual_aligned, y_pred_actual_aligned)
                rmse_test = np.sqrt(mean_squared_error(y_test_actual_aligned, y_pred_actual_aligned))
                mape_test = mean_absolute_percentage_error(y_test_actual_aligned, y_pred_actual_aligned) * 100
            else:
                 print("    Peringatan: Tidak ada data test yang cocok setelah alignment untuk evaluasi.")
                 eval_status = 'Warning: Eval Alignment Failed'

        except Exception as eval_err:
            print(f"    Error saat evaluasi atau inverse transform: {eval_err}")
            eval_status = f"Error: Evaluation Failed ({type(eval_err).__name__})"


        end_time_exp = time.time()
        duration_exp = end_time_exp - start_time_exp
        print(f"   MAE Test : {mae_test:.2f}")
        print(f"   RMSE Test: {rmse_test:.2f}")
        print(f"   MAPE Test: {mape_test:.2f}%")
        print(f"   Durasi   : {duration_exp:.1f} detik")

        # 9. Simpan Hasil Log
        all_results_log.append({
            'Target': target_column,
            'MAE': mae_test,
            'RMSE': rmse_test,
            'MAPE': mape_test,
            'Best_Epoch': best_epoch,
            'Duration_s': duration_exp,
            'Status': eval_status,
            'Model_File': model_filename,
            'Scaler_File': scaler_filename
        })

    # --- Selesai Loop Kolom ---
    print("\n==================================================================")
    print("Semua Proses Training & Penyimpanan Selesai.")

    if not all_results_log:
        print("Tidak ada hasil untuk ditampilkan atau disimpan.")
        return None

    results_log_df = pd.DataFrame(all_results_log)
    results_log_df.sort_values(by='MAPE', inplace=True)

    print("\nLog Hasil Training & Evaluasi (Diurutkan berdasarkan MAPE):")
    with pd.option_context('display.max_rows', 10, 'display.max_columns', None):
        print(results_log_df)

    results_log_path = os.path.join(base_folder_path, results_filename)
    try:
        print(f"\nMenyimpan log hasil ke: {results_log_path}")
        results_log_df.to_csv(results_log_path, index=False)
        print("Log berhasil disimpan.")
    except Exception as e:
        print(f"Gagal menyimpan log: {e}")

    return results_log_df

# --- Cara Menggunakan Fungsi ---

# 1. Muat Data Wide yang Sudah Diimputasi
print("Memuat data wide imputed...")
wide_parquet_path = os.path.join(folder_path, 'data_pangan_jabodetabek_wide_imputed.parquet')
if os.path.exists(wide_parquet_path):
    df_wide_imputed = pd.read_parquet(wide_parquet_path)
    df_wide_imputed.index = pd.to_datetime(df_wide_imputed.index)
    print(f"Data berhasil dimuat. Shape: {df_wide_imputed.shape}")
else:
    print(f"Error: File {wide_parquet_path} tidak ditemukan!")
    df_wide_imputed = None

# (OPSIONAL) Jalankan pada subset kolom untuk testing cepat
# columns_to_run = df_wide_imputed.columns[:5] # Ambil 5 kolom pertama
# df_subset = df_wide_imputed[columns_to_run]
# print(f"\nMenjalankan training hanya pada subset {len(columns_to_run)} kolom...")
# if df_subset is not None:
#      training_log = train_and_save_univariate_models(df_subset, look_back=30, epochs=5) # Epochs lebih sedikit


# 2. Jalankan Training & Penyimpanan untuk SEMUA kolom (Akan SANGAT LAMA!)
if df_wide_imputed is not None:
     print("\nMemulai Eksekusi Fungsi Training & Penyimpanan untuk SEMUA Kolom...")
     training_log = train_and_save_univariate_models(
         df_wide_imputed=df_wide_imputed,
         look_back=60, # Sesuaikan hyperparameter jika perlu
         lstm_units=50,
         epochs=50,
         early_stopping_patience=10
         # Anda bisa menambahkan parameter lain di sini jika perlu
     )
     print("\nFungsi Training & Penyimpanan Selesai.")
     if training_log is not None:
         print("\nDataFrame Log Hasil:")
         print(training_log.head(10)) # Tampilkan 10 hasil terbaik
else:
     print("\nTidak dapat menjalankan training karena data tidak dimuat.")

"""# **==== MULTI-OUTPUT (LONG-TERM Forecast) ====**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau # Checkpoint lagi jika mau simpan model multi-output
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import os
import time
import joblib
import re

folder_path = r'/content/drive/MyDrive/project02'

# --- Konfigurasi BARU ---
HORIZON = 30 # Jumlah hari ke depan yang ingin diprediksi secara langsung
LOOK_BACK = 60 # Jumlah hari historis sebagai input (bisa sama atau beda dari univariate)

# --- Helper Function: Create Multi-Output Sequences ---
def create_multi_output_sequences(data_scaled, n_steps_in, n_steps_out):
    """
    Mengubah data time series menjadi format sequence untuk Direct Multi-Output LSTM.
    Args:
        data_scaled (np.array): Array target/fitur input (scaled). Shape: [n_samples, 1]
        n_steps_in (int): Jumlah langkah waktu input (look_back).
        n_steps_out (int): Jumlah langkah waktu output (horizon).
    Returns:
        np.array: Array X [n_sequences, n_steps_in, 1]
        np.array: Array y [n_sequences, n_steps_out]
    """
    X, y = [], []
    # Iterasi data, berhenti 'n_steps_in + n_steps_out' sebelum akhir
    for i in range(len(data_scaled)):
        # Cari akhir dari pola input
        end_ix = i + n_steps_in
        # Cari akhir dari pola output
        out_end_ix = end_ix + n_steps_out
        # Pastikan kita tidak melewati batas data
        if out_end_ix > len(data_scaled):
            break
        # Ambil input dan output
        seq_x = data_scaled[i:end_ix]
        seq_y = data_scaled[end_ix:out_end_ix] # Ambil sequence output
        X.append(seq_x)
        y.append(seq_y)
    # y perlu di-reshape dari (samples, horizon, 1) menjadi (samples, horizon) jikaDense(units=horizon)
    return np.array(X), np.array(y).reshape(len(y), n_steps_out)


# --- Fungsi Training & Evaluasi Multi-Output (Mirip sebelumnya, tapi modifikasi) ---
def train_evaluate_multi_output_lstm(
    df_wide_imputed,
    target_column, # Target spesifik yang akan diproses
    # -- Parameter --
    train_split_prop=0.7, val_split_prop=0.15,
    look_back=LOOK_BACK, horizon=HORIZON,
    lstm_units=50, dropout_rate=0.2, optimizer='adam', loss_function='mean_squared_error',
    epochs=50, batch_size=32, early_stopping_patience=10,
    base_folder_path='/content/drive/MyDrive/project02',
    models_save_subfolder='multi_output_models_saved', # Folder baru
    save_model_and_scaler=True # Set False jika hanya ingin evaluasi
    ):
    """
    Melatih, mengevaluasi, dan (opsional) menyimpan model LSTM Direct Multi-Output.
    """
    print(f"\n--- Memproses Target: {target_column} (Horizon={horizon}) ---")
    start_time_exp = time.time()

    # Persiapan Nama File
    safe_col_name = re.sub(r'[\\/ ]', '_', target_column)[:80]
    model_filename = f"model_multiout{horizon}d_{safe_col_name}.keras"
    scaler_filename = f"scaler_multiout{horizon}d_{safe_col_name}.gz"
    models_folder = os.path.join(base_folder_path, models_save_subfolder)
    os.makedirs(models_folder, exist_ok=True)
    model_filepath = os.path.join(models_folder, model_filename)
    scaler_filepath = os.path.join(models_folder, scaler_filename)

    result_log = {'Target': target_column, 'Horizon': horizon, 'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan,
                  'Best_Epoch': np.nan, 'Duration_s': 0, 'Status': 'Started',
                  'Model_File': model_filename if save_model_and_scaler else None,
                  'Scaler_File': scaler_filename if save_model_and_scaler else None}

    try:
        # 1. Ambil Data
        ts_data = df_wide_imputed[[target_column]].copy() # Ambil sebagai DataFrame

        # 2. Split Data
        n_total = len(ts_data)
        n_train = int(n_total * train_split_prop)
        n_val = int(n_total * val_split_prop)
        train_data = ts_data.iloc[:n_train]
        val_data = ts_data.iloc[n_train : n_train + n_val]
        test_data = ts_data.iloc[n_train + n_val:]
        print(f"   Split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}")

        # 3. Scaling Data
        scaler = MinMaxScaler(feature_range=(0, 1))
        train_scaled = scaler.fit_transform(train_data) # Input adalah DataFrame
        val_scaled = scaler.transform(val_data)
        test_scaled = scaler.transform(test_data)
        print("   Scaling OK.")

        # Simpan scaler jika diminta
        if save_model_and_scaler:
            try:
                joblib.dump(scaler, scaler_filepath)
                print(f"   Scaler disimpan ke: {scaler_filename}")
            except Exception as e:
                print(f"   Peringatan: Gagal menyimpan scaler: {e}")
                result_log['Status'] = 'Warning: Scaler Save Failed'


        # 4. Membuat Sequence Multi-Output
        print("   Membuat Sequence Multi-Output...")
        X_train_seq, y_train_seq_multi = create_multi_output_sequences(train_scaled, look_back, horizon)
        X_val_seq, y_val_seq_multi = create_multi_output_sequences(val_scaled, look_back, horizon)
        X_test_seq, y_test_seq_scaled = create_multi_output_sequences(test_scaled, look_back, horizon)

        if X_train_seq.shape[0] == 0 or X_val_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:
            print("   Error: Tidak cukup data untuk sequencing. Melewati.")
            result_log['Status'] = 'Error: Insufficient Data for Sequencing'
            result_log['Duration_s'] = time.time() - start_time_exp
            return result_log
        print("   Sequencing OK.")
        print(f"   Shapes: X_train={X_train_seq.shape}, y_train={y_train_seq_multi.shape}, X_test={X_test_seq.shape}, y_test={y_test_seq_scaled.shape}")


        # 5. Bangun & Kompilasi Model Multi-Output
        tf.keras.backend.clear_session()
        model_name_safe = f"LSTM_MultiOut{horizon}d_{safe_col_name}"
        model = Sequential(name=model_name_safe)
        model.add(Input(shape=(look_back, 1))) # Input tetap univariate
        model.add(LSTM(units=lstm_units))
        model.add(Dropout(dropout_rate))
        # --> PERUBAHAN KUNCI: Output units = horizon <--
        model.add(Dense(units=horizon, name='Output_Layer')) # Output sebanyak horizon hari
        model.compile(optimizer=optimizer, loss=loss_function, metrics=['mean_absolute_error'])
        print(f"   Model '{model_name_safe}' dibangun & dikompilasi.")
        model.summary()

        # 6. Siapkan Callbacks
        es = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True, verbose=0)
        callbacks_list = [es]
        if save_model_and_scaler:
            mc = ModelCheckpoint(filepath=model_filepath, monitor='val_loss', save_best_only=True, verbose=0)
            callbacks_list.append(mc)
        # ReduceLROnPlateau bisa ditambahkan jika perlu

        # 7. Latih Model
        print("   Melatih Model...")
        history = model.fit(X_train_seq, y_train_seq_multi,
                            epochs=epochs,
                            batch_size=batch_size,
                            validation_data=(X_val_seq, y_val_seq_multi),
                            callbacks=callbacks_list,
                            verbose=0)
        best_epoch = np.argmin(history.history['val_loss']) + 1
        if save_model_and_scaler:
             print(f"   Training selesai. Best epoch: {best_epoch}. Model terbaik disimpan.")
        else:
             print(f"   Training selesai. Best epoch: {best_epoch}.")


        # 8. Evaluasi Model (Gunakan model terakhir/terbaik dari ES)
        print("   Mengevaluasi model pada Test Set...")
        y_pred_scaled = model.predict(X_test_seq)

        # Inverse Transform (Scaler hanya punya 1 fitur output, perlu trik)
        # Kita inverse transform per langkah waktu prediksi
        y_pred_actual = scaler.inverse_transform(y_pred_scaled) # Shape: (n_samples, horizon)
        y_test_actual = scaler.inverse_transform(y_test_seq_scaled) # Shape: (n_samples, horizon)

        # Hitung Metrik (Rata-rata di seluruh horizon)
        mae_test = mean_absolute_error(y_test_actual.flatten(), y_pred_actual.flatten())
        rmse_test = np.sqrt(mean_squared_error(y_test_actual.flatten(), y_pred_actual.flatten()))
        mape_test = mean_absolute_percentage_error(y_test_actual.flatten(), y_pred_actual.flatten()) * 100

        end_time_exp = time.time()
        duration_exp = end_time_exp - start_time_exp
        print(f"   MAE Test (Avg over {horizon} days): {mae_test:.2f}")
        print(f"   RMSE Test(Avg over {horizon} days): {rmse_test:.2f}")
        print(f"   MAPE Test(Avg over {horizon} days): {mape_test:.2f}%")
        print(f"   Durasi   : {duration_exp:.1f} detik")

        # Update log hasil
        result_log.update({
            'MAE': mae_test, 'RMSE': rmse_test, 'MAPE': mape_test,
            'Best_Epoch': best_epoch, 'Duration_s': duration_exp, 'Status': 'Completed'
        })

    except Exception as e:
        print(f"  FATAL Error saat memproses {target_column}: {e}")
        import traceback
        traceback.print_exc()
        result_log['Status'] = f"Error: {type(e).__name__}"
        result_log['Duration_s'] = time.time() - start_time_exp

    return result_log

# --- Cara Menggunakan Fungsi ---

# 1. Muat Data Wide
print("Memuat data wide imputed...")
wide_parquet_path = os.path.join(folder_path, 'data_pangan_jabodetabek_wide_imputed.parquet')
if os.path.exists(wide_parquet_path):
    df_wide_imputed = pd.read_parquet(wide_parquet_path)
    df_wide_imputed.index = pd.to_datetime(df_wide_imputed.index)
    print(f"Data dimuat. Shape: {df_wide_imputed.shape}")
else:
    print(f"Error: File {wide_parquet_path} tidak ditemukan!")
    df_wide_imputed = None

# 2. Pilih Kolom Target untuk Dilatih (Bisa semua atau subset)
if df_wide_imputed is not None:
    # Contoh: Latih hanya untuk beberapa kolom kunci
    """
    targets_to_train = [
        'Jakarta Pusat_Beras Kualitas Medium I',
        'Kota Bogor_Beras Kualitas Medium I',
        'Jakarta Pusat_Cabai Merah Keriting'
        # Tambahkan target lain di sini
    ]
    """
    # Atau untuk semua kolom (HATI-HATI WAKTU TRAINING LAMA!)
    targets_to_train = df_wide_imputed.columns.tolist()

    all_experiment_logs = []

    print(f"\nMemulai training model Multi-Output (Horizon={HORIZON}) untuk {len(targets_to_train)} target...")

    for target in targets_to_train:
        if target in df_wide_imputed.columns:
            log = train_evaluate_multi_output_lstm(
                df_wide_imputed=df_wide_imputed,
                target_column=target,
                look_back=LOOK_BACK, # Gunakan konstanta
                horizon=HORIZON,     # Gunakan konstanta
                epochs=50,           # Sesuaikan jika perlu
                save_model_and_scaler=True # Set True untuk menyimpan
            )
            all_experiment_logs.append(log)
        else:
            print(f"Peringatan: Target '{target}' tidak ditemukan di data, dilewati.")

    # --- Kumpulkan & Simpan Log Hasil ---
    if all_experiment_logs:
        final_log_df = pd.DataFrame(all_experiment_logs)
        final_log_df.sort_values(by='MAPE', inplace=True)

        print("\n==================================================================")
        print(f"Log Hasil Training & Evaluasi Model Multi-Output (Horizon {HORIZON} Hari):")
        with pd.option_context('display.max_rows', None, 'display.max_columns', None):
             print(final_log_df)

        base_folder_path='/content/drive/MyDrive/project02'
        log_filename = f"lstm_multiout{HORIZON}d_training_log.csv"
        log_path = os.path.join(base_folder_path, log_filename)
        try:
            print(f"\nMenyimpan log hasil ke: {log_path}")
            final_log_df.to_csv(log_path, index=False)
            print("Log berhasil disimpan.")
        except Exception as e:
            print(f"Gagal menyimpan log: {e}")
    else:
        print("\nTidak ada model yang berhasil dilatih/dievaluasi.")
else:
    print("\nTidak dapat memulai training karena data tidak dimuat.")

print("\nProses multi-output selesai.")

